---
title: "A Gentle Introduction to Working in Python"
subtitle: "TDS-F Group Meeting"
format: 
  html:
    toc: true
    toc-expand: 1
    toc-title: Python Tips!
    code-fold: false
    code-overflow: wrap
    theme: yeti
    smooth-scroll: true
jupyter: python3
author: 
    - name: "Tane Kafle"
    - email: "tanuj-kafle@hotmail.co.uk"
---

<!--
format: 
  html:
    toc: true
    toc-expand: 1
    toc-title: Python Tips!
    code-fold: false
    code-overflow: wrap
    theme: yeti
    smooth-scroll: true
-->

<!--
format: 
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    code-overflow: wrap
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}

change _quarto.yml to `output-dir: pdfversion` from `output-dir: docs`.
--->

In this [document](https://tkjmk.github.io/PyBestPractices_SIB/PythonIntro_SIB.html), I will provide an introduction to working in Python - from setting up an environment to making packages. I will try to highlight the quirks of Python and the most useful packages! 

## TIP 1: Work in a Python environment
Below I will show you how to set up a Python environment - the first step is choosing between environment managers: `venv` and `conda`.\

Working in an environment will help you develop your code in a controlled setting. You will use a package manager to install the packages that are required to run your programme, and they will only be available within this environment. How to install packages is also shown in the coding block below. These steps must be done in shell.

::: {.panel-tabset}
### venv
```{.bash}
python3 -m venv sc_env # create environment
source sc_env/bin/activate # enter environment
pip install pandas numpy scanpy # installing packages
```
### conda
```{.bash}
conda create -n sc_env python=3.10 # create environment
conda activate sc_env # enter environment
conda install pandas numpy scanpy # installing packages
```
:::

Once your project is ready to share, you can simply document all the dependencies to run your project using:

::: {.panel-tabset}
### venv
```{.bash}
pip freeze > requirements.txt
```
### conda
```{.bash}
conda env export > environment.yml
```
:::


For me, I prefer using venv with pip - but many people like conda which was designed for data science and [can do a bit more](https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/) than just being a package installer. It can even manage software stacks beyond Python. conda comes packaged with Python distributions such as Anaconda or miniconda, which basically provide you with many of the packages required for scientific analyses - it is typically slower than just using venv and pip to install packages.

## TIP 2: How you import packages will determine how you use them in your scripts

When writing your script there are a few ways to import packages into Python.\
You can just import the package directly using `import package`, all functions will be available as `package.function()`.\
You can provide a new name for the package by importing it as `import package as pkg`, and functions will be available as `pkg.function()`\
You can also directly import a specific function from a package e.g. `import function from package`, and you can use it as `function()`.

```{python}
import sys
import numpy as np # allows you to use numpy functions as np.function_name
import pandas as pd
import scanpy as sc
from os import getcwd # importing a specific function from a package, can be used as getcwd() now.
import timeit
```

## TIP 3: Everything in Python is an object - each type has its own attributes and methods!

Python is an object-oriented language: everything in Python is an object.\
Each object belongs to a type (/class) and comes with its own attributes (describing the object) and methods (actions the object can perform).

For example, a string like `"ACT1"` is an object of type `str`.
It has methods, such as `.lower()`, which converts the string to lowercase:` "ACT1".lower()` returns `"act1"`.
It also has attributes — for example, the string `"ACT1"` has a length of 4. 

Methods are normally accessed like `obj.method()` and attributes by `obj.attribute` (the difference is one has `()` and the other doesn't). 

::: {.callout-note}
In Python you can't actually get the length of a string using `"ACT1".length`, and you are required to use the inbuilt function `len()` i.e. `len("ACT1")`, which will return `4`.
:::

Two essential object types in Python programming are functions and classes, both of which also exist in R.\
They allow you to define custom behavior and create your own object types. Below, I show how they're typically defined and used.

```{python}
# Function
def scale_counts(expmat):
    '''
    These are called docstrings and are placed between three single quotes.
    Here, you can put your guide on how to use a function.
    It can be called by using help(scale_counts)

    This function scales the expression matrix.

    Parameters
    ----------
    expmat : np.ndarray
        Expression matrix in cell x gene format with count data for expression.

    Returns
    -------
    np.ndarray
        Expression matrix standardised based on the matrix sum.
    '''
    return expmat / expmat.sum(axis=1, keepdims=True)

# Class
class Cell:
    def __init__(self, id, expression, location):
        '''
        init function is required for class, it is where you can hold all the data that needs to be input when building this object.
        In this case it will take id, expression and location, and store them as attributes.
        Parameters
        ----------
        id : str
            Identifier for the cell.
        expression : dict
            Dictionary of {gene_name: expression_value}.
        location : str
            String stating where the cell was sampled from.
        '''
        # attributes
        self.id = id
        self.expression = expression
        self.location = location
        
    # methods
    def get_expression(self, gene):
        '''Return the expression level of a single gene.'''
        return self.expression.get(gene, np.nan)  # returns np.nan if gene missing

    def plot_expression(self, gene1, gene2):
        '''Placeholder for a plotting function comparing two genes.'''
        pass  # you can implement later

    def remove_zero_expression(self, gene):
        '''Placeholder for a function that removes genes with zero expression.'''
        pass

    def get_max_expression(self, gene):
        '''Placeholder for a function that tells you gene(s) with highest expression.'''
        pass


# setting up object of my custom class Cell, called mycell
cell1_exp = {'gene1': 2, 'gene2': 3, 'gene3': 0, 'gene4': 8} # dict of expression to input under expression argument of cell.
mycell = Cell('cell1', cell1_exp, 'brain')


print(mycell.get_expression('gene2'))  # using a method of my custom class Cell - should return 3
print(mycell.location)  # accessing an attribute of my custom class Cell - hould return brain

```

You want to use classes when you need reusable, modular code - where you want your data to be in a specific format and you want to apply the same functions (methods) to it. This is what the scanpy authors did when building the AnnData class.

In the above block, I demonstrate the use of docstrings - so that others and most importantly you, can remember what the function/class is. I have used the 'numpy' version of docstrings, the most popular option is the google format. Docstrings can be accessed by using `help(my_function_name)`

## TIP 4: You can turn your scripts with Classes/Functions into packages easily
What is nice, is when you have a series of functions/classes you find yourself using a lot, you can easily package it and import it into other projects of yours.

Let's say I saved the above coding block in a file called "cellutils.py", I would be able to import it into my other scripts using:

```{python}
#| eval: true
# if python cannot find your package put the following line above:
#sys.path.insert(0, '/path/to/dir/where/your/script/is/')
import cellutils as cu

cu.scale_counts(np.array([[1, 2], [3, 4]]))

```

::: {.callout-note}
I will not go into it in too much detail here, but it is best practice to put code that runs your script (i.e. everything that is not defining a class or function) within `if __name__ == "__main__":`. Briefly, when you call the script to run from Shell - it will run your entire script (i.e. `__name__ = "__main__"`), when importing it as a package, it will only run your functions/classes defined outside of this if statement (i.e. `__name__ != "__main__"`).
:::

If you want to run your script as a command line tool, that is also possible and you can look into the package `argparse` to clearly input arguments e.g.

in the command line you could have something like:

```{bash}

python myscript.py -a file.txt -s 0.45 --method x2

```

The Python script would look something like:

```{python}
#| eval: false

#!/usr/bin/env python3
import argparse

def main():
    parser = argparse.ArgumentParser(description="Process a file with a given method and scale.")
    parser.add_argument("-a", "--afile", type=str, required=True, help="Input file (e.g., file.txt)")
    parser.add_argument("-s", "--scale", type=float, required=True, help="Scaling factor (e.g., 0.45)")
    parser.add_argument("--method", type=str, choices=["x1", "x2", "log"], default="x1", help="Processing method")
    
    args = parser.parse_args()

    print(f"File: {args.afile}")
    print(f"Scale: {args.scale}")
    print(f"Method: {args.method}")

if __name__ == "__main__":
    main()

```

## TIP 5: Use comprehensions instead of for loops if your output is a list (or set/dictionary)

In Python the classic `for` and `while` loops exist to go through collection objects (such as lists).

Let's say we have a Python list with genes that are found expressed in a cell:

```{python}
expressed_genes = [
    "Actb",
    "Gapdh",
    "Cd3e",
    "Pax6",
    "Foxp3",
    "Tubb3",
    "mt-Co1",   # mitochondrial
    "mt-Nd2",   # mitochondrial
    "mt-Cytb",  # mitochondrial
    "Il2ra"
]

# if we want to get and print the first letter of each gene
for gene in expressed_genes: # with this line we are iterating through each item in the list expressed_genes - in each iteration of the loop the item will be under the variable gene
  print(gene[0]) # in the for loop we are printing the first letter of the string.

```

::: {.callout-note}
You can remove a variable in python using `del varname`.
:::

In Python, as mentioned before, different object types have their own methods (and attributes) associated with them.\
Below, we have the string `"Actb"` - if we want to see what attributes and methods are available we can use the `dir(var)` function.\
It will print all the attributes and methods available.
```{python}
dir("Actb")
```

If we want to see what any of these do specifically, we can use the `help` function as shown below:

```{python}
help("Actb".startswith)
```


Now we know what the `startswith` method does - lets use it in a for loop to go through the list of expressed genes and identify mitochondrial genes (and store them in a list).

```{python}

mtgenes = [] # initialising empty list
for gene in expressed_genes:    # looping through expressed_genes list
  if gene.startswith('mt-'):    # if the gene starts with mt-
    mtgenes.append(gene)            ## let's append it to the mtgenes list we intiated above.

# how to print to console
print(mtgenes)

# to print a bit nicer
print(", ".join(mtgenes))

# to print even more clearly, we can use f strings - allows us to be more verbose.
print(f'Mitochondrial genes: {", ".join(mtgenes)}')
print(f'There are {len(mtgenes)} mitochondrial genes in the dataset.')

```

### list comprehensions

In general if you are iterating through a list, and the result of your loop is a list - you should use a **list comprehension**!
The reason to do this is because they are more efficient and quicker to do the same analysis than a loop.

::: {.callout-note}
There are also generators, dictionary and set comprehensions - read more about it [here](https://www.geeksforgeeks.org/comprehensions-in-python/).
:::

In this block below, I demonstrate how we can convert the loops shown above into list comprehensions.

```{python}

firstletter_genes = [gene[0] for gene in expressed_genes] # what you want in final list, your iterator id, what you are iterating through

mtgenes_lc = [gene for gene in expressed_genes if gene.startswith('mt')] # you can also add if statements in

print(", ".join(firstletter_genes))
print(", ".join(mtgenes_lc))
```


is it faster??

```{python}
#| code-fold: true
#| code-summary: "Measuring the speed of list comprehensions with timeit:"

setup_code = "expressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']"

loop_code = """
mtgenes = [] # initialising empty list
for gene in expressed_genes:
  if gene.startswith('mt-'):
    mtgenes.append(gene)
"""

list_comp_code = "[gene for gene in expressed_genes if gene.startswith('mt')]"


# Timing both
loop_time = timeit.timeit(loop_code, setup=setup_code, number=10000)
list_comp_time = timeit.timeit(list_comp_code, setup=setup_code, number=10000)

print(f"For loop time: {loop_time:.4f} secs")
print(f"List comprehension time: {list_comp_time:.4f} secs")
```

The reason is list comprehensions essentially run the iterations at a faster and more efficient level using the underlying C implementation, whereas the loop is having to enter the list each time and append to an unknown sized list.

```{python}
#| code-fold: true
#| code-summary: "A simple  way to measure the speed of your code:"

import time

t0 = time.time()
# place code you want to run here!
t1 = time.time()

total = t1-t0 # this will store the time it takes for you to run that code

```

## TIP 6: Sometimes it will be better to use collection objects other than lists!

Beyond `lists`, other collection objects exist. I will briefly show you `sets`, `tuples` and `dictionaries`.

```{python}
# Lists are denoted by []   -> ordered, changeable, can hold duplicates
genes = ["Actb", "Gapdh", "Cd3e"]
# Tuples are denoted by ()  -> ordered, unchangeable, can hold duplicates - makes code faster
genes = ("Actb", "Gapdh", "Cd3e")
# Sets are denoted by {}    -> unordered, immutable (can add/remove items though), duplicates not allowed   
genes = {"Actb", "Gapdh", "Cd3e"}

# to look up the methods available to your data type you can use the dir(genes) or help(genes) function.

# Dictionaries are mappings of keys to values, allowing you to associate names with data. They are also denoted by {}, the items within are not just a list of items, but key: item.
gene_expression = {
    "Actb": 7.2,
    "Gapdh": 8.1,
    "Cd3e": 3.4
}

# you will be able to access any of those variables using your key e.g. gene_expression['Actb'] will return 7.2.

# dictionaries are good to store most object types e.g. you could have a list of genes expressed in key cells
genes_expressed = {
    "Cell1": ['gene1', 'gene2'],
    "Cell2": ['gene1', 'gene3', 'gene4'],
    "Cell3": ['gene2', 'gene3']
}

# you can even have dictionaries of dictionaries!
# dictionaries are really useful for storing data - you could have multiple dicts e.g. genes_mutated, gene_lengths and you can access the data for a particular sample with the same key e.g. genes_mutated['sample1'] or gene_lengths['sample1'].

# Side note: {} will intialise an enpty dictionary ([] for list and () for tuples). To intialise an empty set, you must use set().

```

In general, I use lists. I use sets occasionally, for example, to hold a list of genes that I need to filter out of a dataset (I don't care about order here, don't want duplicates, and won't really be changing it).

Dictionaries I use relatively frequently, especially for storing data/metadata - and if you have a dictionary of (equally sized) lists, it is easy to convert it to a pandas dataframe (which I will go into data layer).

Another usecase of a dictionary would be if you have a list of mouse genes and their associated human genes - you will be able to easily convert a list of mouse genes to human genes using the dictionary.


### Indexing and Slicing

Below, I show some ways to index and slice a list, for your reference.

```{python}

expressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']

print("first item of genes list:") # Python is zero-indexed (i.e., indexing starts at 0)!
expressed_genes[0]

print("items 1 to 3 of genes list:")
expressed_genes[1:3]

print("last item of genes list:")
expressed_genes[-1]

print("every second item in the genes list:")
expressed_genes[::2]


if 'Gapdh' in expressed_genes: 
    print('Gapdh is expressed')
else:
    print('Gapdh is not expressed')

if 'Lyve1' not in expressed_genes: 
    print('Lyve1 is not expressed')
elif 'Lyve1' in expressed_genes:
    print('Lyve1 is expressed')
else:
    print('Lyve1 is neither expressed nor not expressed - impossible!')


print('Get length of a variable:')
len(expressed_genes)

# methods available for lists:
expressed_genes.sort() # it sorts in place, so no need to put it inot a new variable.

expressed_genes.append('Lyve1') # genes.insert(0, 'Lyve1') if you want to insert it at the beginning

expressed_genes.remove('Lyve1')


```


## TIP 7: Catch your errors using try except

Sometimes you want to do an operation, but there is an expected error that might pop up - normally an error will cause your programme to exit - you might not want this behaviour. With errors you expect, you can catch the error and do something other than exiting - this is achieved by using `try except`./
For example, if you have genes you always want to remove from a dataset of expressed genes - you might have a fixed list of genes to remove. However, in a future project, those genes may not be expressed and therefore not in the list of expressed genes -> trying to remove it will raise an error -> you can catch the error and do something else like printing a helpful warning message (or run a different function).

```{python}

genes2remove = ['Pins', 'Pard3']


for gene in genes2remove:
    try:
        expressed_genes.remove(gene)
    except ValueError:
        print(f'{gene} not an expressed_gene, so cannot be removed')


```



## TIP 8: Use pandas to read in and handle DataFrames

```{python}
#| code-fold: true
#| code-summary: "Generating the tables:"
#| eval: false
import os

np.random.seed(42)

# generating a 10x10 table with 10 genes and 10 cells
genes = [f"Gene{i}" for i in range(10)]
cells = [f"Cell{i}" for i in range(10)]

# random count matrix (integers between 50 and 500)
count_matrix = np.random.randint(50, 500, size=(10, 10))
df = pd.DataFrame(count_matrix, index=genes, columns=cells)


df_KO = df.copy() # creating KO version: copying to modify
# reducing expression for Gene3 and Gene7 in Cell0, Cell1, Cell2
for gene in ["Gene3", "Gene7"]:
    for cell in ["Cell0", "Cell1", "Cell2"]:
        df_KO.loc[gene, cell] = df_KO.loc[gene, cell] // 10  # Strong reduction


output_dir = '/Users/tkafle/Documents/PyBestPractices_SIB/data/' # ensuring output dir exists
os.makedirs(output_dir, exist_ok=True)
# saving the files
df.to_csv(f"{output_dir}/WT_1.csv")
random_changes = np.random.randint(-50, 51, size=df.shape)
modified_df = df + random_changes
modified_df = modified_df.where(df >= 0, 0) # could alternatively use modified_df.clip(lower=0)
modified_df.to_csv(f"{output_dir}/WT_2.csv")
df_KO.to_csv(f"{output_dir}/KO_1.csv")
random_changes = np.random.randint(-50, 51, size=df_KO.shape)
modified_df = df_KO + random_changes
modified_df = modified_df.where(modified_df >= 0, 0) # could alternatively use modified_df.clip(lower=0)
modified_df.to_csv(f"{output_dir}/KO_2.csv")

```

The most commonly used package to handle tables in Python is `pandas` oftened imported as `pd`.

It is easy to read in a csv into pandas using it's built in read_csv function:
```{python}
import os
output_dir = os.path.join(os.getcwd(), 'data')
wt1_df = pd.read_csv(f"{output_dir}/WT_1.csv", index_col=0) # the table is of type: pd.DataFrame
```


Now you have your data stored in an object of type `pd.DataFrame` - and of course there are a bunch of methods and attributes you can now access to inspect and process the dataframe. I highlight some examples below.

```{python}

# Attributes
print("Shape of dataframe:")
print(wt1_df.shape)    # (10, 10) — 10 genes x 10 cells

print("Column names of dataframe:")
print(wt1_df.columns)  # list of cell names (columns)

# Methods
print("\nTransposing dataframe:")
print(wt1_df.transpose().head()) # flip genes and cells
print("\nAdding a new column:")
wt1_df['GeneSum'] = wt1_df.sum(axis=1) # add a new column: sum of counts per gene
print(wt1_df.head())
wt1_df.drop(['GeneSum'], axis=1, inplace=True) # removing the new column, #inplace lets us do it directly on the df and not create a new variable.

# Change data type (example: ensure all counts are integers)
wt1_df = wt1_df.astype(int)

# remember you can use dir(pd.DataFrame) and the help() function to better understand the attributes/methods of this object type.

```

Below, I briefly show ways you can access data in your `pd.DataFrame`.
```{python}
#| eval: false

# USING NAMES OF COLUMNS AND ROWS
# accessing the expression of a speficific gene in a specific cell
wt1_df.loc['Gene0', 'Cell1'] # row name, column name

# accessing the expression values of a specific cell
wt1_df['Cell1'] # column name

# accessing the expression of a specific row
wt1_df.loc['Gene0'] # row name

# accessing multiple columns of a specific row
wt1_df.loc['Gene0', wt1_df.columns.str.startswith('Cell')]


# USING INDEX VALUES OF COLUMNS AND ROWS
# accessing a specific cell 
wt1_df.iloc[0, 1] # row index, column index

# accessing a row
wt1_df.iloc[4, :] # row index (usually you would simply do: wt1_df.iloc[4])

# accessing a column
wt1_df.iloc[:, 0] # : means whole row, column index


```


In an AnnData object, a lot of the data is stored in pandas dataframes (e.g. var and obs).

## TIP 9: Use NumPy for advanced mathematical functions

You can perform basic mathematical functions in Python, not dissimilarly to R. Some advanced functions are in numpy (usually imported as np). Basic mathematical functions:
```{python}

# Mathematical functions available directly in Python
# addition
_ = 5 + 4

# subtraction
_ = 5 - 4

# multiplication
_ = 5 * 4

# division
_ =  5 / 2

# squaring
_ =  5 ** 2

# getting remainder (modulo)
_ =  5 % 2

# Advanced functions in Python are largely found in packages such as numpy [np]

# square root
_ = np.sqrt(16)  # or math.sqrt(16)

# natural logarithm (log base e)
_ = np.log(10)  # or math.log(10)

# logarithm base 10
_ = np.log10(100)

# exponentiation (e^x)
_ = np.exp(4)

# absolute value
_ = np.abs(-5)

# rounding numbers
_ = np.round(3.567, 2)  # rounds to 2 decimal places



# I will note here, when printing long floats, f strings are really useful for formatting them.

num = 3.1415926535
# round to 3 decimal places
print(f"Pi rounded to 3 decimals: {num:.3f}") # f means 
# round to 1 decimal place
print(f"Pi rounded to 1 decimal: {num:.1f}")

sml_num = 0.00123456
# 3 significant figures in decimals
print(f"{sml_num:.3g}") #g means general format
big_num = 123456
# 3 significant figures (scientific notication)
print(f"{big_num:.3g}")

```

Remember when printing floats:

- `.3f` = 3 decimal places.
- `.3g` = 3 significant figures.

`np` is also better for doing mathematical functions across vectors and things like matrix maths.

## TIP 10: Several packages exist for plotting: seaborn and matplotlib can probably do most of what you want.

In general there are three popular libraries:

- Matplotlib (basic, customisable, but more work needed to build plots) - most similar to base R plotting
- Seaborn (easy, beautiful defaults) - most similar to ggplot2
- Plotly (interactive) - also exists in R!


The dataset I will use is the data I generated earlier in this document - it contains 2 samples of KO and 2 samples of WT with count data from 10 genes and 10 cells.

```{python}
#| code-fold: true
#| code-summary: "Reading in the dataset:"

# load the datasets
wt1 = pd.read_csv(f"{output_dir}/WT_1.csv", index_col=0)
wt2 = pd.read_csv(f"{output_dir}/WT_2.csv", index_col=0)
ko1 = pd.read_csv(f"{output_dir}/KO_1.csv", index_col=0)
ko2 = pd.read_csv(f"{output_dir}/KO_2.csv", index_col=0)

# stack into one DataFrame for easier plotting
# adding group labels
wt1['group'] = 'WT'
wt2['group'] = 'WT'
ko1['group'] = 'KO'
ko2['group'] = 'KO'
wt1['id'] = 'WT1'
wt2['id'] = 'WT2'
ko1['id'] = 'KO1'
ko2['id'] = 'KO2'

# Combine into one DataFrame
df = pd.concat([wt1, wt2, ko1, ko2])

print(df.head())
```

### matplotlib.pyplot (plt)

Here, I will show how to make a plot and layer using matplotlib.pyplot (plt).
The plot will be a histogram showing the expression of genes in Cell2 for sample 1.

`matplotlib.pyplot` is the underlying library that `seaborn` builds on top of. You can keep layering plots onto the same figure until you call `plt.show()`, which renders the final output. If you'd like to create multiple plots arranged in a grid, you can follow this helpful [guide](https://www.geeksforgeeks.org/plot-multiple-plots-in-matplotlib/).

```{python}
import matplotlib.pyplot as plt

cell2_wt1_expression =  df[df.id == 'WT1']['Cell2']
plt.figure(figsize=(6,4)) # initating figure size
plt.hist(cell2_wt1_expression, bins=5, color='skyblue', edgecolor='black') # plotting the histogram
plt.title('Distribution of Gene_1 Expression') # title of plot
plt.xlabel('Expression') # x axis label
plt.ylabel('Count') # y axis label
plt.grid(True)  # layering on grid
plt.axvline(cell2_wt1_expression.mean(), color='red', linestyle='dashed', linewidth=2, label='Mean')  # placing vertical line to show where the mean is
plt.legend() # plot the legend
plt.show() # show the plot

```

`plt.show()` is what you need to do to show the plot in a popup window. If you want to save the figure there is `plt.savefig()`.\
Other good practices are to close figures once they have been initiated using `plt.close()` or `plt.clf()`.

### seaborn (sns)

seaborn usually runs on long data, so we will need to melt the pandas dataframe from before.

```{python}
#| code-fold: true
#| code-summary: "Melting the dataset:"



# melting each DF
if 'gene' not in wt1.columns:
    wt1.reset_index(inplace=True)
    wt1.rename(columns={'index': 'gene'}, inplace=True)
wt1_melted = wt1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')

if 'gene' not in wt2.columns:
    wt2.reset_index(inplace=True)
    wt2.rename(columns={'index': 'gene'}, inplace=True)
wt2_melted = wt2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')

if 'gene' not in ko1.columns:
    ko1.reset_index(inplace=True)
    ko1.rename(columns={'index': 'gene'}, inplace=True)
ko1_melted = ko1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')

if 'gene' not in ko2.columns:
    ko2.reset_index(inplace=True)
    ko2.rename(columns={'index': 'gene'}, inplace=True)
ko2_melted = ko2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')


# Merge the DataFrames
merged_df = pd.concat([wt1_melted, wt2_melted, ko1_melted, ko2_melted], ignore_index=True)
#merged_df['gene'] = merged_df['gene'].apply(lambda x: 'Gene' + str(int(x)))

print(merged_df.head())
```

Seaborn is typically imported as `sns`.

Here, I show an example using seaborn - just requires the long form pd.DataFrame, and you can easily plot the x and y axis based on specific columns. Here, I am plotting a violin plot using `sns.violinplot`, other plot types can be found [here](https://seaborn.pydata.org/api.html).

```{python}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Violin plot: gene 1"
#|   - "Violin plot gene 3"


import seaborn as sns

# Gene1
plt.figure(figsize=(6,4))
sns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene1'], palette='muted')
plt.title('Gene1 Expression by Group')
plt.show()

#Gene3
plt.figure(figsize=(6,4))
sns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene3'], palette='muted')
plt.title('Gene3 Expression by Group')
plt.show()

```

In this example, I am layering using seaborn - plotting a swarmplot above the catplot. I store the catplot in the variable g, I then plot the swarmplot on the same axes by using `ax=g.ax`.
```{python}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Violin plot: gene 2"
#|   - "Violin plot gene 7"

import seaborn as sns

# Gene2
g = sns.catplot(data=merged_df[merged_df.gene == 'Gene2'], x="group", y="expression",
                kind="violin", color=".9", inner=None, height=4, aspect=1.5)
sns.swarmplot(data=merged_df[merged_df.gene == 'Gene2'], x="group", y="expression", size=3, ax=g.ax)
g.ax.set_title('Gene2 Expression by Group')
plt.show()

# gene7
g = sns.catplot(data=merged_df[merged_df.gene == 'Gene7'], x="group", y="expression",
                kind="violin", color=".9", inner=None, height=4, aspect=1.5)
sns.swarmplot(data=merged_df[merged_df.gene == 'Gene7'], x="group", y="expression", size=3, ax=g.ax)
g.ax.set_title('Gene7 Expression by Group')
plt.show()


# you can access the data in the sns.FacetGrid object (variable g in my script) by calling g.data
```


In the example below I use `sns.scatterplot` to plot a volcano plot. I generate some fake data.\
I use seaborn for the scatter plot, and then layer on top of it using `matplotlib.pyplot` (`plt`).
```{python}
# creating fake log2 fold change and p-value data
volcano_data = pd.DataFrame({
    #'gene': ['Gene0', 'Gene1', 'Gene2', 'Gene3', 'Gene4', 'Gene5', 'Gene6', 'Gene7', 'Gene8', 'Gene9'],
    'log2FC': np.random.randn(1000),  # Random fold changes
    'pval': np.random.rand(1000)  # Random p-values
})

# calculating -log10(p-value)
volcano_data['-log10(pval)'] = -np.log10(volcano_data['pval'])

plt.figure(figsize=(6,5))
sns.scatterplot(data=volcano_data, x='log2FC', y='-log10(pval)', color='purple')
plt.axhline(y=1.3, color='red', linestyle='dashed')  # threshold line for p-value = 0.05
plt.axvline(x=0, color='black', linestyle='dotted')
plt.title('Volcano Plot (Mock Example)')
plt.xlabel('Log2 Fold Change')
plt.ylabel('-Log10(p-value)')
plt.show()


```


### plotly

Plotly exists in R as well, and is good for interactivity - widget-like plots. You can hover over points and filter points by clicking on the legend. 

plotly express functions are not too dissimilar to using seabron - in the example below I use `px.strip` to generate a strip plot. It requires long data format `pd.DataFrame`, and you plot simply by putting the columns you want to plot under x and y (columns group and expression, respectively). The colour of each point is based on the column cell.

```{python}

import plotly.express as px

# Plotly: Strip Plot (interactive)
fig = px.strip(merged_df[merged_df.gene == 'Gene3'], x='group', y='expression', color='cell', stripmode='overlay', title='Gene Expression by Cell (WT vs KO)')
fig.show()

# the variable fig is a plotly.graph_objects.Figure
```




## TIP 11: You can do your statistical tests in python using scipy.stats or statsmodels

R is known for being able to carry out stats tests, but through packages such as statsmodels and scipy.stats, we have access to many statistical tests that you need for scientific studies.


I will show you some examples, these tests are shown only for demonstration; there's no real hypothesis being tested.

### GLM
```{python}
import statsmodels.api as sm


#merged_df['group_binary'] = merged_df['group'].apply(lambda x: 1 if x == 'KO' else 0) # converting KO/WT to binary 0/1.

# for this example, we will just put gene expression as a continuous response
X = pd.get_dummies(merged_df['group'], drop_first=True)  # one-hot encoding for group variable
X = sm.add_constant(X)  # add intercept
y = merged_df['expression']

# Fit GLM
model = sm.GLM(y, X, family=sm.families.Gaussian()).fit()

# Show the results
print(model.summary())



```


### T-test
```{python}
from scipy import stats

# two conditions WT vs KO, different in expression
ko_expr = merged_df[merged_df['group'] == 'KO']['expression']
wt_expr = merged_df[merged_df['group'] == 'WT']['expression']

# performing two-sample t-test
t_stat, p_value = stats.ttest_ind(ko_expr, wt_expr)

print(f"T-statistic: {t_stat}, P-value: {p_value}")


```

### Chi2 test
```{python}


# categorising gene expression into 'high' or 'low' based on median expression
median_expr = merged_df['expression'].median()
merged_df['expr_category'] = merged_df['expression'].apply(lambda x: 'high' if x > median_expr else 'low')

# creating a contingency table for chisq test
contingency = pd.crosstab(merged_df['expr_category'], merged_df['group'])

# performing the Chi-Squared test
chi2, p_val, dof, expected = stats.chi2_contingency(contingency)

print(f"Chi-Squared: {chi2}, P-value: {p_val}")


```

### ANOVA
```{python}

# performing ANOVA on expression values grouped by the 'group' column
group1 = merged_df[merged_df['group'] == 'WT']['expression']
group2 = merged_df[merged_df['group'] == 'KO']['expression']

f_stat, p_val = stats.f_oneway(group1, group2)
print(f"ANOVA F-statistic: {f_stat}, P-value: {p_val}")


```


### Running correction on multiple testing
```{python}

import pandas as pd
import scipy.stats as stats
from statsmodels.stats.multitest import multipletests


# creating a list to store p-values - we will run multiple test correction
p_values = []

# iterating over each gene (rows of the dataframe) to get expression for KO and WT
for g in merged_df.gene.unique():
    # getting expression data for the gene in both KO and WT groups
    ko_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'KO')]['expression'].to_numpy()
    wt_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'WT')]['expression'].to_numpy()

    # performing two-sample t-test on each gene expression data
    t_stat, p_val = stats.ttest_ind(ko_expr, wt_expr, equal_var=False)
    p_values.append(p_val)

# Now we will apply multiple testing correction to the p-values using Benjamini-Hochberg (FDR) method, other methods such as Bonferroni are available
reject, corrected_pvals, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')

# Create a dataframe to view the results
results = pd.DataFrame({
    'gene': merged_df.gene.unique(),
    'p-value': p_values,
    'corrected p-value': corrected_pvals,
    'reject null hypothesis': reject
})

print(results)


```



## TIP 12: You can parallelise your code using multiprocessing

```{python}
import time
import multiprocessing


def calculate_square(number):
    '''
    function to calculate square of a number (CPU-bound task)
    '''
    return number * number


def parallel_square(numbers):
    '''
    function to run the calculation in parallel using multiple processes
    '''
    # you need to create a pool of worker processes
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool: # line that lets you access your cpus to run script
        result = pool.map(calculate_square, numbers)
    return result

def single_threaded_square(numbers):
    '''
    single-threaded version: Calculating squares sequentially
    '''
    result = []
    for number in numbers:
        result.append(calculate_square(number))
    return result

if __name__ == "__main__":
    numbers = [x for x in range(1, 10000001)]  # 10 million numbers
    
    # measuring the time for the single-threaded execution
    start_time = time.time()
    single_result = single_threaded_square(numbers)
    single_thread_time = time.time() - start_time
    print(f"Single-threaded time: {single_thread_time:.2f} seconds")
    
    # measuring the time for the parallel execution
    start_time = time.time()
    parallel_result = parallel_square(numbers)
    parallel_time = time.time() - start_time
    print(f"Parallel execution time: {parallel_time:.2f} seconds")
    
    # comparing results to make sure they are the same
    print(f"Do the results match? {'Yes' if single_result == parallel_result else 'No'}")




```

## TIP 13: You can generate pipelines using bash/python/r with Snakemake

show basic snakemake document with three rules: cellranger [bash], preprocessing [python], plotting [r]

```{python}
#| eval: false

# Snakefile

# Define the rule for Cell Ranger processing
rule cellranger:
    input:
        r1="data/{sample}_R1.fastq.gz",
        r2="data/{sample}_R2.fastq.gz"
    output:
        "output/cellranger_output/{sample}/filtered_feature_bc_matrix"
    shell:
        "cellranger count --id={wildcards.sample} "
        "--fastqs={input.r1},{input.r2} "
        "--transcriptome=/path/to/refdata-cellranger-mm10-3.0.0 "
        "--sample={wildcards.sample}"

# Define the rule for Python data processing
rule process_data:
    input:
        "output/cellranger_output/{sample}/filtered_feature_bc_matrix/matrix.mtx"
    output:
        "output/analysis_results/{sample}_processed.csv"
    script:
        "scripts/process_data.py"

# Define the rule for generating plots using R
rule plot_data:
    input:
        "output/analysis_results/{sample}_processed.csv"
    output:
        "output/analysis_results/{sample}_plot.png"
    script:
        "scripts/plot_data.R"


```

run workflow with `snakemake --cores 4 -s Snakefile` 



## TIP 14: Use type hinting to write clearly usable functions and unit testing to ensure your programmes work as expected.

In the example below, it shows the function normalise requires the argument `x` and this should be of type `np.ndarray`, and the `-> np.ndarray`, shows the expected output should be of type `np.ndarray` - helping you or someone else know what to expect from this function.
```{python}
def normalise(x: np.ndarray) -> np.ndarray:
    return x / x.sum()

```

Other common types:
`int`, `float`, `list`, `tuple`, `range`, `str`, `set`, `dict`, `bool`, `NoneType`, `pd.Series`, `pd.DataFrame`, `np.ndarray`, `np.int64`, `np.float64`, `sc.AnnData`.


Unit testing is a crucial practice, especially when your code is intended for use by others or will be reused in the future. It involves writing small tests that check whether individual functions behave as expected given specific inputs. These tests help catch bugs early, document expected behavior, and ensure that future changes don’t break existing functionality. Implementing unit tests improves code reliability and maintainability. You can use `unittest` or `pytest` for this.
```{python}
#| eval: false

import unittest
import numpy as np

class TestNormaliseFunction(unittest.TestCase):

    def test_normalise_single_dimension(self):
        """Test normalization of a 1D array."""
        x = np.array([1, 2, 3])
        expected = np.array([0.16666667, 0.33333333, 0.5])  # sum = 6, so divide each element by 6
        result = normalise(x)
        np.testing.assert_array_almost_equal(result, expected, decimal=8)
    
    def test_normalise_multiple_dimension(self):
        """Test normalization of a 2D array."""
        x = np.array([[1, 2], [3, 4]])
        expected = np.array([[0.1, 0.2], [0.3, 0.4]])  # sum = 10, divide each element by 10
        result = normalise(x)
        np.testing.assert_array_almost_equal(result, expected, decimal=8)

    def test_normalise_edge_case(self):
        """Test normalization of an edge case (empty array)."""
        x = np.array([])
        with self.assertRaises(ValueError):  # expecting a ValueError if sum is 0
            normalise(x)

if __name__ == "__main__":
    unittest.main()
    
```



## TIP 15: Use IPython when writing your code to more easily test it and debug it.

**IPython**  allows you to interactively code, so you can test your code in your terminal.


## R & Python together

- In Python you can do a lot of the vectorisation programming you do in R using numpy and pandas.

- Python will have a bit of a learning curve for plots/stats - some R only packages (especially scientific) may not be available.

- Use both! (Snakemake helps bridge them when building pipelines) 


## Working with AnnData objects

In a later document, I will demonstrate how to analyse single-cell RNAseq data using Python.


