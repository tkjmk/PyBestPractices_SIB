[
  {
    "objectID": "PythonIntro_SIB.html",
    "href": "PythonIntro_SIB.html",
    "title": "A Gentle Introduction to Working in Python",
    "section": "",
    "text": "In this document, I will provide an introduction to working in Python - from setting up and environment to making packages. I will try to highlight the quirks of Python and the most useful packages!"
  },
  {
    "objectID": "PythonIntro_SIB.html#setting-up-your-environment",
    "href": "PythonIntro_SIB.html#setting-up-your-environment",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Setting up your environment",
    "text": "Setting up your environment\nThe first step is choosing between environment managers: venv and conda. These will help you work in a contained environment and will allow you to use package managers to download Python packages.\n\nvenvconda\n\n\npython3 -m venv sc_env # create environment\nsource sc_env/bin/activate # enter environment\npip install pandas numpy scanpy # installing packages\n\n\nconda create -n sc_env python=3.10 # create environment\nconda activate sc_env # enter environment\nconda install pandas numpy scanpy # installing packages\n\n\n\nOnce your project is ready to share, you can simply document all the dependencies to run your project using:\n\nvenvconda\n\n\npip freeze &gt; requirements.txt\n\n\nconda env export &gt; environment.yml\n\n\n\nFor me, I prefer using venv with pip - but many people like conda which was designed for data science and can do a bit more than just being a package installer and can even manage software stacks beyond Python. conda comes packaged with Anaconda or miniconda, which basically provide you with many of the packages required for scientific analyses."
  },
  {
    "objectID": "PythonIntro_SIB.html#importing-packages",
    "href": "PythonIntro_SIB.html#importing-packages",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Importing packages",
    "text": "Importing packages\nWhen writing your script there are a few ways to import packages into Python.\nYou can just import the package directly using import package, all functions will be available as package.function().\nYou can provide a new name for the package by importing it as import package as pkg, and functions will be available as pkg.function()\nYou can also directly import a specific function from a package e.g. import function from package, and you can use it as function().\n\nimport sys\nimport numpy as np # allows you to use numpy functions as np.function_name\nimport pandas as pd\nimport scanpy as sc\nfrom os import getcwd # importing a specific function from a package, can be used as getcwd() now.\nimport timeit"
  },
  {
    "objectID": "PythonIntro_SIB.html#an-object-oriented-language",
    "href": "PythonIntro_SIB.html#an-object-oriented-language",
    "title": "A Gentle Introduction to Working in Python",
    "section": "An object-oriented language",
    "text": "An object-oriented language\nPython is object-oriented: everything focuses around objects - everything pretty much is an object.\nThese objects have their own attributes (i.e. their characteristics) and methods (i.e. things they can do).\nFor example, for a string a method would be that you can lowercase the whole word e.g. “ACT1” to “act1” by \"ACT\".lower().\nAn attribute a string has for example is its length for example “ACT1” has a length of 4 characters.\n\nTwo objects that are important for Python programming are functions and classes - which also exist in R. Below, I show how they are typically in formatted.\n\n# Function\ndef scale_counts(expmat):\n    '''\n    These are called docstrings and are placed between three single quotes.\n    Here, you can put your guide on how to use a function.\n    It can be called by using help(scale_counts)\n\n    This function scales the expression matrix.\n\n    Parameters\n    ----------\n    expmat : np.ndarray\n        Expression matrix in cell x gene format with count data for expression.\n\n    Returns\n    -------\n    np.ndarray\n        Expression matrix standardised based on the matrix sum.\n    '''\n    return expmat / expmat.sum(axis=1, keepdims=True)\n\n# Class\nclass Cell:\n    def __init__(self, id, expression, location):\n        '''\n        Parameters\n        ----------\n        id : str\n            Identifier for the cell.\n        expression : dict\n            Dictionary of {gene_name: expression_value}.\n        '''\n        self.id = id\n        self.expression = expression\n        self.location = location\n        \n    def get_expression(self, gene):\n        '''Return the expression level of a single gene.'''\n        return self.expression.get(gene, np.nan)  # returns np.nan if gene missing\n\n    def plot_expression(self, gene1, gene2):\n        '''Placeholder for a plotting function comparing two genes.'''\n        pass  # you can implement later\n\n    def remove_zero_expression(self, gene):\n        '''Placeholder for a function that removes genes with zero expression.'''\n        pass\n\n    def get_max_expressin(self, gene):\n        '''Placeholder for a function that tells you gene(s) with highest expression.'''\n        pass\n\n\n# Example usage\ncell1_exp = {'gene1': 2, 'gene2': 3, 'gene3': 0, 'gene4': 8}\n\nmycell = Cell('cell1', cell1_exp, 'brain')\nprint(mycell.get_expression('gene2'))  # Should return 3\nprint(mycell.location)  # Should return brain\n\n3\nbrain\n\n\nYou want to use classes when you need reusable, modular code - where you want to apply the same functions. This is what the scanpy authors did when building the AnnData object.\nIn the above block, I demonstrate the use of docstrings - so that others and most importantly you, can remember what the function/class is."
  },
  {
    "objectID": "PythonIntro_SIB.html#making-your-own-package",
    "href": "PythonIntro_SIB.html#making-your-own-package",
    "title": "A Gentle Introduction to Working in Python",
    "section": "making your own package",
    "text": "making your own package\nWhat is nice, is when you have a series of functions/classes you find yourself using a lot, you can easily package it and import it into other projects.\nLet’s say I saved the above coding block in a file called “cellutils.py”, I would be able to import it into my other scripts using:\n\n# if python cannot find your package put the following line above:\n#sys.path.insert(0, '/path/to/dir/where/your/script/is/')\nimport cellutils as cu\n\ncu.scale_counts(np.array([[1, 2], [3, 4]]))\n\narray([[0.33333333, 0.66666667],\n       [0.42857143, 0.57142857]])\n\n\nIf you want to run your script as a command line tool, that is also possible and you can look into argparse to input arguments e.g.\nin the command line you could have something like:\n\npython myscript.py -a file.txt -s 0.45 --method x2\n\nThe Python script would look something like:\n\n#!/usr/bin/env python3\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Process a file with a given method and scale.\")\n    parser.add_argument(\"-a\", \"--afile\", type=str, required=True, help=\"Input file (e.g., file.txt)\")\n    parser.add_argument(\"-s\", \"--scale\", type=float, required=True, help=\"Scaling factor (e.g., 0.45)\")\n    parser.add_argument(\"--method\", type=str, choices=[\"x1\", \"x2\", \"log\"], default=\"x1\", help=\"Processing method\")\n    \n    args = parser.parse_args()\n\n    print(f\"File: {args.afile}\")\n    print(f\"Scale: {args.scale}\")\n    print(f\"Method: {args.method}\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "PythonIntro_SIB.html#loops-and-comprehensions",
    "href": "PythonIntro_SIB.html#loops-and-comprehensions",
    "title": "A Gentle Introduction to Working in Python",
    "section": "loops and comprehensions",
    "text": "loops and comprehensions\nIn Python the classic for and while loops exist to go through collection objects.\nLet’s say we have a Python list with genes that are found expressed in a cell:\n\nexpressed_genes = [\n    \"Actb\",\n    \"Gapdh\",\n    \"Cd3e\",\n    \"Pax6\",\n    \"Foxp3\",\n    \"Tubb3\",\n    \"mt-Co1\",   # mitochondrial\n    \"mt-Nd2\",   # mitochondrial\n    \"mt-Cytb\",  # mitochondrial\n    \"Il2ra\"\n]\n\n# if we want to get and print the first letter of each gene\nfor gene in expressed_genes: # with this line we are iterating through each item in the list expressed_genes\n  print(gene[0]) # in the for loop we are printing the first letter of the string.\n\nA\nG\nC\nP\nF\nT\nm\nm\nm\nI\n\n\nIn python different types of objects have methods associated with them. In this example we have a list of strings - if we want to see what attributes and methods are available we use dir(var)\n\ndir(\"Actb\")\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nIf we want to see what these methods do, we can use the help function.\n\nhelp(\"Actb\".startswith)\n\nHelp on built-in function startswith:\n\nstartswith(...) method of builtins.str instance\n    S.startswith(prefix[, start[, end]]) -&gt; bool\n    \n    Return True if S starts with the specified prefix, False otherwise.\n    With optional start, test S beginning at that position.\n    With optional end, stop comparing S at that position.\n    prefix can also be a tuple of strings to try.\n\n\n\nWith this startswith method for strings, lets loop through the list of expressed genes and identify mitochondrial genes (then store them in a list).\n\nmtgenes = [] # initialising empty list\nfor gene in expressed_genes:\n  if gene.startswith('mt-'):\n    mtgenes.append(gene)\n\nprint(mtgenes)\n# to print a bit nicer\nprint(\", \".join(mtgenes))\n\n# to print even more clearly, we can use f strings\nprint(f'Mitochondrial genes: {\", \".join(mtgenes)}')\nprint(f'There are {len(mtgenes)} mitochondrial genes in the dataset.')\n\n['mt-Co1', 'mt-Nd2', 'mt-Cytb']\nmt-Co1, mt-Nd2, mt-Cytb\nMitochondrial genes: mt-Co1, mt-Nd2, mt-Cytb\nThere are 3 mitochondrial genes in the dataset.\n\n\n\nlist comprehensions\nIn general if you are iterating through a list, and the result of your loop is a list - you should use a list comprehension! The reason to do this is because they are more efficient and quicker to do the same analysis than a loop.\nIn this block below, I demonstrate how we can convert the loops shown above into list comprehensions.\n\nfirstletter_genes = [gene[0] for gene in expressed_genes] # what you want in final list, your iterator id, what you are iterating through\n\nmtgenes_lc = [gene for gene in expressed_genes if gene.startswith('mt')] # you can also add if statements in\n\nprint(\", \".join(firstletter_genes))\nprint(\", \".join(mtgenes_lc))\n\nA, G, C, P, F, T, m, m, m, I\nmt-Co1, mt-Nd2, mt-Cytb\n\n\nis it faster??\n\n\nMeasuring the speed of list comprehensions with timeit:\nsetup_code = \"expressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']\"\n\nloop_code = \"\"\"\nmtgenes = [] # initialising empty list\nfor gene in expressed_genes:\n  if gene.startswith('mt-'):\n    mtgenes.append(gene)\n\"\"\"\n\nlist_comp_code = \"[gene for gene in expressed_genes if gene.startswith('mt')]\"\n\n\n# Timing both\nloop_time = timeit.timeit(loop_code, setup=setup_code, number=10000)\nlist_comp_time = timeit.timeit(list_comp_code, setup=setup_code, number=10000)\n\nprint(f\"For loop time: {loop_time:.4f} secs\")\nprint(f\"List comprehension time: {list_comp_time:.4f} secs\")\n\n\nFor loop time: 0.0147 secs\nList comprehension time: 0.0144 secs\n\n\nThe reason is list comprehensions essentially run the iterations at a faster level using the underhood of Python (C), whereas the loop is having to enter the list each time and append to an unknown sized list.\n\n\nA simple way to measure the speed of your code:\nimport time\n\nt0 = time.time()\n# place code you want to run here!\nt1 = time.time()\n\ntotal = t1-t0 # this will store the time it takes for you to run that code\n\n\n\n\nGoing beyond lists\nBeyond lists, other collection objects exist sets, tuples and dictionaries.\n\n# Lists are denoted by []   -&gt; ordered, changeable, can hold duplicates\ngenes = [\"Actb\", \"Gapdh\", \"Cd3e\"]\n# Tuples are denoted by ()  -&gt; ordered, unchangeable, can hold duplicates - makes code faster\ngenes = (\"Actb\", \"Gapdh\", \"Cd3e\")\n# Sets are denoted by {}    -&gt; unordered, immutable (can add/remove items though), duplicates not allowed   \ngenes = {\"Actb\", \"Gapdh\", \"Cd3e\"}\n\n# to look up the methods available to your data type you can use the dir(genes) or help(genes) function.\n\n# Dictionaries are kind of a named list also denoted by {}, but within the items will have keys.\ngene_expression = {\n    \"Actb\": 7.2,\n    \"Gapdh\": 8.1,\n    \"Cd3e\": 3.4\n}\n# these are good to store items e.g. you could have a list of genes epxressed in key cells\ngenes_expressed = {\n    \"Cell1\": ['gene1', 'gene2'],\n    \"Cell2\": ['gene1', 'gene3', 'gene4'],\n    \"Cell3\": ['gene2', 'gene3']\n}\n\n# you can even have dictionaries of dictionaries!\n\nIn general, I use lists. I use sets occasionally, for example, to hold a list of genes that I need to filter out of a dataset (I don’t care about order here, don’t want duplicates, and won’t really be changing it).\nDictionaries I use relatively frequently, especially for storing data/metadata - and if you have a dictionary of lists, it is easy to convert it to a pandas dataframe. Another usecase of a dictionary would be if you have a list of mouse genes and their associated human genes, it will allow for easy humanisation of the genes.\n\n\nIndexing and Slicing\n\nexpressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']\n\nprint(\"first item of genes list:\") #Python is 0-indexed!\nexpressed_genes[0]\n\nprint(\"items 1 to 3 of genes list:\")\nexpressed_genes[1:3]\n\nprint(\"last item of genes list:\")\nexpressed_genes[-1]\n\nprint(\"every second item in the genes list:\")\nexpressed_genes[::2]\n\n\nif 'Gapdh' in expressed_genes: \n    print('Gapdh is expressed')\nelse:\n    print('Gapdh is not expressed')\n\nif 'Lyve1' not in expressed_genes: \n    print('Lyve1 is not expressed')\nelif 'Lyve1' in expressed_genes:\n    print('Lyve1 is expressed')\nelse:\n    print('Lyve1 is neither expressed nor not expressed - impossible!')\n\n\nprint('Get length of a variable:')\nlen(expressed_genes)\n\n# methods available for lists:\nexpressed_genes.sort()\n\nexpressed_genes.append('Lyve1') # genes.insert(0, 'Lyve1') if you want to insert it at the beginning\n\nexpressed_genes.remove('Lyve1')\n\nfirst item of genes list:\nitems 1 to 3 of genes list:\nlast item of genes list:\nevery second item in the genes list:\nGapdh is expressed\nLyve1 is not expressed\nGet length of a variable:\n\n\n\n\nTry Except\nSometimes you want to do an operation, but there is an expected error that might pop up - you might not want your program to exit but to catch that error and do something else - this can be achieved with try except./ For example, if you have genes you always want to remove from a dataset of expressed genes - you might have a fixed list of genes to remove. However, in a future study, those genes may not be expressed and therefore not in the list of expressed genes -&gt; trying to remove it will raise an error -&gt; you can catch the error and do something else like printing a helpful message.\n\ngenes2remove = ['Pins', 'Pard3']\n\n\nfor gene in genes2remove:\n    try:\n        expressed_genes.remove(gene)\n    except ValueError:\n        print(f'{gene} not an expressed_gene, so cannot be removed')\n\nPins not an expressed_gene, so cannot be removed\nPard3 not an expressed_gene, so cannot be removed"
  },
  {
    "objectID": "PythonIntro_SIB.html#reading-in-tables",
    "href": "PythonIntro_SIB.html#reading-in-tables",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Reading in Tables",
    "text": "Reading in Tables\n\n\nGenerating the tables:\nimport os\n\nnp.random.seed(42)\n\n# generating a 10x10 table with 10 genes and 10 cells\ngenes = [f\"Gene{i}\" for i in range(10)]\ncells = [f\"Cell{i}\" for i in range(10)]\n\n# random count matrix (integers between 50 and 500)\ncount_matrix = np.random.randint(50, 500, size=(10, 10))\ndf = pd.DataFrame(count_matrix, index=genes, columns=cells)\n\n\ndf_KO = df.copy() # creating KO version: copying to modify\n# reducing expression for Gene3 and Gene7 in Cell0, Cell1, Cell2\nfor gene in [\"Gene3\", \"Gene7\"]:\n    for cell in [\"Cell0\", \"Cell1\", \"Cell2\"]:\n        df_KO.loc[gene, cell] = df_KO.loc[gene, cell] // 10  # Strong reduction\n\n\noutput_dir = '/Users/tkafle/Documents/PyBestPractices_SIB/data/' # ensuring output dir exists\nos.makedirs(output_dir, exist_ok=True)\n# saving the files\ndf.to_csv(f\"{output_dir}/WT_1.csv\")\nrandom_changes = np.random.randint(-50, 51, size=df.shape)\nmodified_df = df + random_changes\nmodified_df = modified_df.where(df &gt;= 0, 0)\nmodified_df.to_csv(f\"{output_dir}/WT_2.csv\")\ndf_KO.to_csv(f\"{output_dir}/KO_1.csv\")\nrandom_changes = np.random.randint(-50, 51, size=df_KO.shape)\nmodified_df = df_KO + random_changes\nmodified_df = modified_df.where(modified_df &gt;= 0, 0)\nmodified_df.to_csv(f\"{output_dir}/KO_2.csv\")\n\n\nThe most commonly used package to handle tables in Python is pandas oftened imported as pd.\nIt is easy to read in a csv into pandas using:\n\noutput_dir = '/Users/tkafle/Documents/PyBestPractices_SIB/data/'\nwt1_df = pd.read_csv(f\"{output_dir}/WT_1.csv\", index_col=0) # the table is of type: pd.DataFrame\n\nYou can then do classic things you might want to do attirbutes methods:\n\n# Attributes\nprint(\"Shape of dataframe:\")\nprint(wt1_df.shape)    # (10, 10) — 10 genes x 10 cells\n\nprint(\"Column names of dataframe:\")\nprint(wt1_df.columns)  # list of cell names (columns)\n\n# Methods\nprint(\"\\nTransposing dataframe:\")\nprint(wt1_df.transpose().head()) # flip genes and cells\nprint(\"\\nAdding a new column:\")\nwt1_df['GeneSum'] = wt1_df.sum(axis=1) # add a new column: sum of counts per gene\nprint(wt1_df.head())\nwt1_df.drop(['GeneSum'], axis=1, inplace=True) # removing the new column, #inplace lets us do it directly on the df and not create a new variable.\n\n# Change data type (example: ensure all counts are integers)\nwt1_df = wt1_df.astype(int)\n\n# remember you can use dir(pd.DataFrame) and help()\n\nShape of dataframe:\n(10, 10)\nColumn names of dataframe:\nIndex(['Cell0', 'Cell1', 'Cell2', 'Cell3', 'Cell4', 'Cell5', 'Cell6', 'Cell7',\n       'Cell8', 'Cell9'],\n      dtype='object')\n\nTransposing dataframe:\n       Gene0  Gene1  Gene2  Gene3  Gene4  Gene5  Gene6  Gene7  Gene8  Gene9\nCell0    152    264    307     71    495    184    314    480    103    495\nCell1    485    380    393    302    224     70    395     84    155    319\nCell2    398    137    463    285    495    378    102    255    309    400\nCell3    320    422    343    394    100    216    435    130    359    353\nCell4    156    149    435     98    413    323    389    469    240    320\n\nAdding a new column:\n       Cell0  Cell1  Cell2  Cell3  Cell4  Cell5  Cell6  Cell7  Cell8  Cell9  \\\nGene0    152    485    398    320    156    121    238     70    152    171   \nGene1    264    380    137    422    149    409    201    180    199    358   \nGene2    307    393    463    343    435    241    493    326    210    363   \nGene3     71    302    285    394     98    108    219    237    320    239   \nGene4    495    224    495    100    413    104    293    369    180    356   \n\n       GeneSum  \nGene0     2263  \nGene1     2699  \nGene2     3574  \nGene3     2273  \nGene4     3029  \n\n\n\n# acessing the expression of a speficific gene in a specific cell\nwt1_df.loc['Gene0', 'Cell1'] # row name, column name\n\n# accessing the expression values of a specific cell\nwt1_df['Cell1'] # column name\n\n# acessing the expresson of a specific row\nwt1_df.loc['Gene0'] # row name\n\n# accessing multiple columns of a specific row\nwt1_df.loc['Gene0', wt1_df.columns.str.startswith('Cell')]\n\n\n# acessing a specific cell\nwt1_df.iloc[0, 1] # row index, column index\n\n# acessing a row\nwt1_df.iloc[4, :] # row index (usually you would simply do: wt1_df.iloc[4])\n\n# acessing a column\nwt1_df.iloc[:, 0] # : means whole row, column index\n\nIn an AnnData object, a lot of the data is stored in pandas dataframes (e.g. var and obs)."
  },
  {
    "objectID": "PythonIntro_SIB.html#basic-numericalmathematical-functions",
    "href": "PythonIntro_SIB.html#basic-numericalmathematical-functions",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Basic numerical/mathematical functions",
    "text": "Basic numerical/mathematical functions\nYou can perform basic mathematical functions in Python, not dissimilarly to R. Some advanced functions are in numpy (usually imported as np).\n\n# addition\n_ = 5 + 4\n\n# subtraction\n_ = 5 - 4\n\n# multiplication\n_ = 5 * 4\n\n# division\n_ =  5 / 2\n\n# squaring\n_ =  5 ** 2\n\n# getting remainder (modulo)\n_ =  5 % 2\n\n# Advanced functions in Python are largely found in packages such as numpy [np]\n\n# square root\n_ = np.sqrt(16)  # or math.sqrt(16)\n\n# natural logarithm (log base e)\n_ = np.log(10)  # or math.log(10)\n\n# logarithm base 10\n_ = np.log10(100)\n\n# exponentiation (e^x)\n_ = np.exp(4)\n\n# absolute value\n_ = np.abs(-5)\n\n# rounding numbers\n_ = np.round(3.567, 2)  # rounds to 2 decimal places\n\n\n\n# I will note here, when printing long floats, f strings are really useful for formatting them.\n\nnum = 3.1415926535\n# round to 3 decimal places\nprint(f\"Pi rounded to 3 decimals: {num:.3f}\") # f means \n# round to 1 decimal place\nprint(f\"Pi rounded to 1 decimal: {num:.1f}\")\n\nsml_num = 0.00123456\n# 3 significant figures in decimals\nprint(f\"{sml_num:.3g}\") #g means general format\nbig_num = 123456\n# 3 significant figures (scientific notication)\nprint(f\"{big_num:.3g}\")\n\nPi rounded to 3 decimals: 3.142\nPi rounded to 1 decimal: 3.1\n0.00123\n1.23e+05\n\n\nRemember when printing floats: - .3f = 3 decimal places. - .3g = 3 significant figures."
  },
  {
    "objectID": "PythonIntro_SIB.html#plotting",
    "href": "PythonIntro_SIB.html#plotting",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Plotting",
    "text": "Plotting\nIn general there are three popular libraries: - Matplotlib (basic, customizable) - Seaborn (easy, beautiful defaults) - Plotly (interactive)\nThe dataset will be some generated data of KO vs WT counts, with 10 genes and 10 cells.\n\n\nReading in the dataset:\n# load the datasets\nwt1 = pd.read_csv(f\"{output_dir}/WT_1.csv\", index_col=0)\nwt2 = pd.read_csv(f\"{output_dir}/WT_2.csv\", index_col=0)\nko1 = pd.read_csv(f\"{output_dir}/KO_1.csv\", index_col=0)\nko2 = pd.read_csv(f\"{output_dir}/KO_2.csv\", index_col=0)\n\n# stack into one DataFrame for easier plotting\n# adding group labels\nwt1['group'] = 'WT'\nwt2['group'] = 'WT'\nko1['group'] = 'KO'\nko2['group'] = 'KO'\nwt1['id'] = 'WT1'\nwt2['id'] = 'WT2'\nko1['id'] = 'KO1'\nko2['id'] = 'KO2'\n\n# Combine into one DataFrame\ndf = pd.concat([wt1, wt2, ko1, ko2])\n\nprint(df.head())\n\n\n       Cell0  Cell1  Cell2  Cell3  Cell4  Cell5  Cell6  Cell7  Cell8  Cell9  \\\nGene0    152    485    398    320    156    121    238     70    152    171   \nGene1    264    380    137    422    149    409    201    180    199    358   \nGene2    307    393    463    343    435    241    493    326    210    363   \nGene3     71    302    285    394     98    108    219    237    320    239   \nGene4    495    224    495    100    413    104    293    369    180    356   \n\n      group   id  \nGene0    WT  WT1  \nGene1    WT  WT1  \nGene2    WT  WT1  \nGene3    WT  WT1  \nGene4    WT  WT1  \n\n\n\nmatplotlib.pyplot (plt)\nHere, I will show how to make a plot and layer using matplotlib.pyplot (plt). The plot will be a histogram showing the expression of genes in Cell2 for sample 1.\n\nimport matplotlib.pyplot as plt\n\ncell2_wt1_expression =  df[df.id == 'WT1']['Cell2']\nplt.figure(figsize=(6,4)) # initating figure size\nplt.hist(cell2_wt1_expression, bins=5, color='skyblue', edgecolor='black') # plotting the histogram\nplt.title('Distribution of Gene_1 Expression') # title of plot\nplt.xlabel('Expression') # x axis label\nplt.ylabel('Count') # y axis label\nplt.grid(True)  # layering on grid\nplt.axvline(cell2_wt1_expression.mean(), color='red', linestyle='dashed', linewidth=2, label='Mean')  # placing vertical line to show where the mean is\nplt.legend() # plot the legend\nplt.show() # show the plot\n\n\n\n\n\n\n\n\nplt.show() is what you need to do to show the plot in a popup window. If you want to save the figure there is plt.savefig().\nOther good practices are to close figures once they have been initiated using plt.close() or plt.clf().\n\n\nseaborn (sns)\nseaborn usually runs on long data, so we will need to melt the pandas dataframe from before.\n\n\nMelting the dataset:\n# melting each DF\nif 'gene' not in wt1.columns:\n    wt1.reset_index(inplace=True)\n    wt1.rename(columns={'index': 'gene'}, inplace=True)\nwt1_melted = wt1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in wt2.columns:\n    wt2.reset_index(inplace=True)\n    wt2.rename(columns={'index': 'gene'}, inplace=True)\nwt2_melted = wt2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in ko1.columns:\n    ko1.reset_index(inplace=True)\n    ko1.rename(columns={'index': 'gene'}, inplace=True)\nko1_melted = ko1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in ko2.columns:\n    ko2.reset_index(inplace=True)\n    ko2.rename(columns={'index': 'gene'}, inplace=True)\nko2_melted = ko2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\n\n# Merge the DataFrames\nmerged_df = pd.concat([wt1_melted, wt2_melted, ko1_melted, ko2_melted], ignore_index=True)\n#merged_df['gene'] = merged_df['gene'].apply(lambda x: 'Gene' + str(int(x)))\n\nprint(merged_df.head())\n\n\n  group   id   gene   cell  expression\n0    WT  WT1  Gene0  Cell0         152\n1    WT  WT1  Gene1  Cell0         264\n2    WT  WT1  Gene2  Cell0         307\n3    WT  WT1  Gene3  Cell0          71\n4    WT  WT1  Gene4  Cell0         495\n\n\nimport seaborn as sns\n\nplt.figure(figsize=(6,4))\nsns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene1'], palette='muted')\nplt.title('Gene1 Expression by Group')\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene3'], palette='muted')\nplt.title('Gene3 Expression by Group')\nplt.show()\n\n\n\n\n\n\nViolin plot: gene 1\n\n\n\n\n\n\n\nViolin plot gene 3\n\n\n\n\n\nimport seaborn as sns\n\n# Gene2\ng = sns.catplot(data=merged_df[merged_df.gene == 'Gene2'], x=\"group\", y=\"expression\",\n                kind=\"violin\", color=\".9\", inner=None, height=4, aspect=1.5)\nsns.swarmplot(data=merged_df[merged_df.gene == 'Gene2'], x=\"group\", y=\"expression\", size=3, ax=g.ax)\ng.ax.set_title('Gene2 Expression by Group')\nplt.show()\n\ng = sns.catplot(data=merged_df[merged_df.gene == 'Gene7'], x=\"group\", y=\"expression\",\n                kind=\"violin\", color=\".9\", inner=None, height=4, aspect=1.5)\nsns.swarmplot(data=merged_df[merged_df.gene == 'Gene7'], x=\"group\", y=\"expression\", size=3, ax=g.ax)\ng.ax.set_title('Gene7 Expression by Group')\nplt.show()\n\n\n\n\n\n\nViolin plot: gene 2\n\n\n\n\n\n\n\nViolin plot gene 7\n\n\n\n\n\n\n# creating fake log2 fold change and p-value data\nvolcano_data = pd.DataFrame({\n    #'gene': ['Gene0', 'Gene1', 'Gene2', 'Gene3', 'Gene4', 'Gene5', 'Gene6', 'Gene7', 'Gene8', 'Gene9'],\n    'log2FC': np.random.randn(1000),  # Random fold changes\n    'pval': np.random.rand(1000)  # Random p-values\n})\n\n# calculating -log10(p-value)\nvolcano_data['-log10(pval)'] = -np.log10(volcano_data['pval'])\n\nplt.figure(figsize=(6,5))\nsns.scatterplot(data=volcano_data, x='log2FC', y='-log10(pval)', color='purple')\nplt.axhline(y=1.3, color='red', linestyle='dashed')  # threshold line for p-value = 0.05\nplt.axvline(x=0, color='black', linestyle='dotted')\nplt.title('Volcano Plot (Mock Example)')\nplt.xlabel('Log2 Fold Change')\nplt.ylabel('-Log10(p-value)')\nplt.show()"
  },
  {
    "objectID": "PythonIntro_SIB.html#statistics",
    "href": "PythonIntro_SIB.html#statistics",
    "title": "A Gentle Introduction to Working in Python",
    "section": "statistics",
    "text": "statistics\nR is known for its statistical properties, but the stats module from scipy brings alot of that functionality over.\nexample glm, example ttest, example chisq test (maybe from the small 100 row table just imported above)\n\nimport statsmodels.api as sm\n\n\n#merged_df['group_binary'] = merged_df['group'].apply(lambda x: 1 if x == 'KO' else 0) # converting KO/WT to binary 0/1.\n\n# for this example, we will just put gene expression as a continuous response\nX = pd.get_dummies(merged_df['group'], drop_first=True)  # one-hot encoding for group variable\nX = sm.add_constant(X)  # add intercept\ny = merged_df['expression']\n\n# Fit GLM\nmodel = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n\n# Show the results\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             expression   No. Observations:                  400\nModel:                            GLM   Df Residuals:                      398\nModel Family:                Gaussian   Df Model:                            1\nLink Function:               identity   Scale:                          17963.\nMethod:                          IRLS   Log-Likelihood:                -2525.8\nDate:                Tue, 29 Apr 2025   Deviance:                   7.1492e+06\nTime:                        15:14:57   Pearson chi2:                 7.15e+06\nNo. Iterations:                     3   Pseudo R-squ. (CS):           0.002870\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        269.2350      9.477     28.409      0.000     250.660     287.810\nWT            14.3400     13.403      1.070      0.285     -11.928      40.608\n==============================================================================\n\n\n\nfrom scipy import stats\n\n# two conditions WT vs KO, different in expression\nko_expr = merged_df[merged_df['group'] == 'KO']['expression']\nwt_expr = merged_df[merged_df['group'] == 'WT']['expression']\n\n# performing two-sample t-test\nt_stat, p_value = stats.ttest_ind(ko_expr, wt_expr)\n\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nT-statistic: -1.069947991397797, P-value: 0.2852911632028579\n\n\n\n# categorising gene expression into 'high' or 'low' based on median expression\nmedian_expr = merged_df['expression'].median()\nmerged_df['expr_category'] = merged_df['expression'].apply(lambda x: 'high' if x &gt; median_expr else 'low')\n\n# creating a contingency table for chisq test\ncontingency = pd.crosstab(merged_df['expr_category'], merged_df['group'])\n\n# performing the Chi-Squared test\nchi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n\nprint(f\"Chi-Squared: {chi2}, P-value: {p_val}\")\n\nChi-Squared: 0.4900490049004901, P-value: 0.4839054452991407\n\n\n\n# performing ANOVA on expression values grouped by the 'group' column\ngroup1 = merged_df[merged_df['group'] == 'WT']['expression']\ngroup2 = merged_df[merged_df['group'] == 'KO']['expression']\n\nf_stat, p_val = stats.f_oneway(group1, group2)\nprint(f\"ANOVA F-statistic: {f_stat}, P-value: {p_val}\")\n\nANOVA F-statistic: 1.144788704296184, P-value: 0.28529116320296\n\n\nRunning multiple test correction\n\nimport pandas as pd\nimport scipy.stats as stats\nfrom statsmodels.stats.multitest import multipletests\n\n\n# creating a list to store p-values - we will run multiple test correction\np_values = []\n\n# iterating over each gene (rows of the dataframe) to get expression for KO and WT\nfor g in merged_df.gene.unique():\n    # getting expression data for the gene in both KO and WT groups\n    ko_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'KO')]['expression'].to_numpy()\n    wt_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'WT')]['expression'].to_numpy()\n\n    # performing two-sample t-test on each gene expression data\n    t_stat, p_val = stats.ttest_ind(ko_expr, wt_expr, equal_var=False)\n    p_values.append(p_val)\n\n# Now we will apply multiple testing correction to the p-values using Benjamini-Hochberg (FDR) method, other emthods such as Bonferroni are available\nreject, corrected_pvals, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n\n# Create a dataframe to view the results\nresults = pd.DataFrame({\n    'gene': merged_df.gene.unique(),\n    'p-value': p_values,\n    'corrected p-value': corrected_pvals,\n    'reject null hypothesis': reject\n})\n\nprint(results)\n\n    gene   p-value  corrected p-value  reject null hypothesis\n0  Gene0  0.935963           0.995188                   False\n1  Gene1  0.951398           0.995188                   False\n2  Gene2  0.673559           0.995188                   False\n3  Gene3  0.106674           0.995188                   False\n4  Gene4  0.816119           0.995188                   False\n5  Gene5  0.864392           0.995188                   False\n6  Gene6  0.995188           0.995188                   False\n7  Gene7  0.224214           0.995188                   False\n8  Gene8  0.886540           0.995188                   False\n9  Gene9  0.667638           0.995188                   False"
  },
  {
    "objectID": "PythonIntro_SIB.html#parallelism",
    "href": "PythonIntro_SIB.html#parallelism",
    "title": "A Gentle Introduction to Working in Python",
    "section": "parallelism",
    "text": "parallelism\n\nimport time\nimport multiprocessing\n\n\ndef calculate_square(number):\n    '''\n    function to calculate square of a number (CPU-bound task)\n    '''\n    return number * number\n\n\ndef parallel_square(numbers):\n    '''\n    function to run the calculation in parallel using multiple processes\n    '''\n    # you need to reate a pool of worker processes\n    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n        result = pool.map(calculate_square, numbers)\n    return result\n\ndef single_threaded_square(numbers):\n    '''\n    single-threaded version: Calculating squares sequentially\n    '''\n    result = []\n    for number in numbers:\n        result.append(calculate_square(number))\n    return result\n\nif __name__ == \"__main__\":\n    numbers = [x for x in range(1, 10000001)]  # 10 million numbers\n    \n    # measuring the time for the single-threaded execution\n    start_time = time.time()\n    single_result = single_threaded_square(numbers)\n    single_thread_time = time.time() - start_time\n    print(f\"Single-threaded time: {single_thread_time:.2f} seconds\")\n    \n    # measuring the time for the parallel execution\n    start_time = time.time()\n    parallel_result = parallel_square(numbers)\n    parallel_time = time.time() - start_time\n    print(f\"Parallel execution time: {parallel_time:.2f} seconds\")\n    \n    # comparing results to make sure they are the same\n    print(f\"Do the results match? {'Yes' if single_result == parallel_result else 'No'}\")\n\nSingle-threaded time: 1.21 seconds\nParallel execution time: 2.37 seconds\nDo the results match? Yes"
  },
  {
    "objectID": "PythonIntro_SIB.html#snakemake",
    "href": "PythonIntro_SIB.html#snakemake",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Snakemake",
    "text": "Snakemake\nshow basic snakemake document with three rules: cellranger [bash], preprocessing [python], plotting [r]\n\n# Snakefile\n\n# Define the rule for Cell Ranger processing\nrule cellranger:\n    input:\n        r1=\"data/{sample}_R1.fastq.gz\",\n        r2=\"data/{sample}_R2.fastq.gz\"\n    output:\n        \"output/cellranger_output/{sample}/filtered_feature_bc_matrix\"\n    shell:\n        \"cellranger count --id={wildcards.sample} \"\n        \"--fastqs={input.r1},{input.r2} \"\n        \"--transcriptome=/path/to/refdata-cellranger-mm10-3.0.0 \"\n        \"--sample={wildcards.sample}\"\n\n# Define the rule for Python data processing\nrule process_data:\n    input:\n        \"output/cellranger_output/{sample}/filtered_feature_bc_matrix/matrix.mtx\"\n    output:\n        \"output/analysis_results/{sample}_processed.csv\"\n    script:\n        \"scripts/process_data.py\"\n\n# Define the rule for generating plots using R\nrule plot_data:\n    input:\n        \"output/analysis_results/{sample}_processed.csv\"\n    output:\n        \"output/analysis_results/{sample}_plot.png\"\n    script:\n        \"scripts/plot_data.R\"\n\n\nrun workflow with snakemake --cores 4"
  },
  {
    "objectID": "PythonIntro_SIB.html#working-with-anndata",
    "href": "PythonIntro_SIB.html#working-with-anndata",
    "title": "A Gentle Introduction to Working in Python",
    "section": "working with anndata",
    "text": "working with anndata\nI will talk more about this the next time I present a group meeting"
  },
  {
    "objectID": "PythonIntro_SIB.html#typehinting-and-testing",
    "href": "PythonIntro_SIB.html#typehinting-and-testing",
    "title": "A Gentle Introduction to Working in Python",
    "section": "typehinting and testing",
    "text": "typehinting and testing\n\ndef normalize(x: np.ndarray) -&gt; np.ndarray:\n    return x / x.sum()\n\nOther common types: int, float, list, tuple, range, str, set, dict, bool, NoneType, pd.Series, pd.DataFrame, np.ndarray, np.int64, np.float64, sc.AnnData.\nExample unit testing\n\nimport unittest\nimport numpy as np\n\nclass TestNormaliseFunction(unittest.TestCase):\n\n    def test_normalize_single_dimension(self):\n        \"\"\"Test normalization of a 1D array.\"\"\"\n        x = np.array([1, 2, 3])\n        expected = np.array([0.16666667, 0.33333333, 0.5])  # sum = 6, so divide each element by 6\n        result = normalise(x)\n        np.testing.assert_array_almost_equal(result, expected, decimal=8)\n    \n    def test_normalize_multiple_dimension(self):\n        \"\"\"Test normalization of a 2D array.\"\"\"\n        x = np.array([[1, 2], [3, 4]])\n        expected = np.array([[0.1, 0.2], [0.3, 0.4]])  # sum = 10, divide each element by 10\n        result = normalise(x)\n        np.testing.assert_array_almost_equal(result, expected, decimal=8)\n\n    def test_normalize_edge_case(self):\n        \"\"\"Test normalization of an edge case (empty array).\"\"\"\n        x = np.array([])\n        with self.assertRaises(ValueError):  # expecting a ValueError if sum is 0\n            normalize(x)\n\nif __name__ == \"__main__\":\n    unittest.main()"
  },
  {
    "objectID": "PythonIntro_SIB.html#how-do-i-test-all-my-code-interactively",
    "href": "PythonIntro_SIB.html#how-do-i-test-all-my-code-interactively",
    "title": "A Gentle Introduction to Working in Python",
    "section": "how do I test all my code interactively?",
    "text": "how do I test all my code interactively?\niPython\niPython allows you to interactively code, so you can test your code in your terminal."
  },
  {
    "objectID": "PythonIntro_SIB.html#r-python",
    "href": "PythonIntro_SIB.html#r-python",
    "title": "A Gentle Introduction to Working in Python",
    "section": "🧵 R & Python",
    "text": "🧵 R & Python\n\n\nyou can do a lot of the vectorisation programming in R using numpy and pandas\nPython will have a bit of a learning curve for plots/stats - some R only packages (especially scientific) may not be available.\nUse both! (Snakemake helps bridge them)"
  }
]