[
  {
    "objectID": "PythonIntro_SIB.html",
    "href": "PythonIntro_SIB.html",
    "title": "A Gentle Introduction to Working in Python",
    "section": "",
    "text": "In this document, I will provide an introduction to working in Python - from setting up an environment to making packages. I will try to highlight the quirks of Python and the most useful packages!"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-1-work-in-a-python-environment",
    "href": "PythonIntro_SIB.html#tip-1-work-in-a-python-environment",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 1: Work in a Python environment",
    "text": "TIP 1: Work in a Python environment\nBelow I will show you how to set up a Python environment - the first step is choosing between environment managers: venv and conda.\n\nWorking in an environment will help you develop your code in a controlled setting. You will use a package manager to install the packages that are required to run your programme, and they will only be available within this environment. How to install packages is also shown in the coding block below. These steps must be done in shell.\n\nvenvconda\n\n\npython3 -m venv sc_env # create environment\nsource sc_env/bin/activate # enter environment\npip install pandas numpy scanpy # installing packages\n\n\nconda create -n sc_env python=3.10 # create environment\nconda activate sc_env # enter environment\nconda install pandas numpy scanpy # installing packages\n\n\n\nOnce your project is ready to share, you can simply document all the dependencies to run your project using:\n\nvenvconda\n\n\npip freeze &gt; requirements.txt\n\n\nconda env export &gt; environment.yml\n\n\n\nFor me, I prefer using venv with pip - but many people like conda which was designed for data science and can do a bit more than just being a package installer. It can even manage software stacks beyond Python. conda comes packaged with Python distributions such as Anaconda or miniconda, which basically provide you with many of the packages required for scientific analyses - it is typically slower than just using venv and pip to install packages."
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-2-how-you-import-packages-will-determine-how-you-use-them-in-your-scripts",
    "href": "PythonIntro_SIB.html#tip-2-how-you-import-packages-will-determine-how-you-use-them-in-your-scripts",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 2: How you import packages will determine how you use them in your scripts",
    "text": "TIP 2: How you import packages will determine how you use them in your scripts\nWhen writing your script there are a few ways to import packages into Python.\nYou can just import the package directly using import package, all functions will be available as package.function().\nYou can provide a new name for the package by importing it as import package as pkg, and functions will be available as pkg.function()\nYou can also directly import a specific function from a package e.g. import function from package, and you can use it as function().\n\nimport sys\nimport numpy as np # allows you to use numpy functions as np.function_name\nimport pandas as pd\nimport scanpy as sc\nfrom os import getcwd # importing a specific function from a package, can be used as getcwd() now.\nimport timeit"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-3-everything-in-python-is-an-object---each-type-has-its-own-attributes-and-methods",
    "href": "PythonIntro_SIB.html#tip-3-everything-in-python-is-an-object---each-type-has-its-own-attributes-and-methods",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 3: Everything in Python is an object - each type has its own attributes and methods!",
    "text": "TIP 3: Everything in Python is an object - each type has its own attributes and methods!\nPython is an object-oriented language: everything in Python is an object.\nEach object belongs to a type (/class) and comes with its own attributes (describing the object) and methods (actions the object can perform).\nFor example, a string like \"ACT1\" is an object of type str. It has methods, such as .lower(), which converts the string to lowercase:\"ACT1\".lower() returns \"act1\". It also has attributes — for example, the string \"ACT1\" has a length of 4.\nMethods are normally accessed like obj.method() and attributes by obj.attribute (the difference is one has () and the other doesn’t).\n\n\n\n\n\n\nNote\n\n\n\nIn Python you can’t actually get the length of a string using \"ACT1\".length, and you are required to use the inbuilt function len() i.e. len(\"ACT1\"), which will return 4.\n\n\nTwo essential object types in Python programming are functions and classes, both of which also exist in R.\nThey allow you to define custom behavior and create your own object types. Below, I show how they’re typically defined and used.\n\n# Function\ndef scale_counts(expmat):\n    '''\n    These are called docstrings and are placed between three single quotes.\n    Here, you can put your guide on how to use a function.\n    It can be called by using help(scale_counts)\n\n    This function scales the expression matrix.\n\n    Parameters\n    ----------\n    expmat : np.ndarray\n        Expression matrix in cell x gene format with count data for expression.\n\n    Returns\n    -------\n    np.ndarray\n        Expression matrix standardised based on the matrix sum.\n    '''\n    return expmat / expmat.sum(axis=1, keepdims=True)\n\n# Class\nclass Cell:\n    def __init__(self, id, expression, location):\n        '''\n        init function is required for class, it is where you can hold all the data that needs to be input when building this object.\n        In this case it will take id, expression and location, and store them as attributes.\n        Parameters\n        ----------\n        id : str\n            Identifier for the cell.\n        expression : dict\n            Dictionary of {gene_name: expression_value}.\n        location : str\n            String stating where the cell was sampled from.\n        '''\n        # attributes\n        self.id = id\n        self.expression = expression\n        self.location = location\n        \n    # methods\n    def get_expression(self, gene):\n        '''Return the expression level of a single gene.'''\n        return self.expression.get(gene, np.nan)  # returns np.nan if gene missing\n\n    def plot_expression(self, gene1, gene2):\n        '''Placeholder for a plotting function comparing two genes.'''\n        pass  # you can implement later\n\n    def remove_zero_expression(self, gene):\n        '''Placeholder for a function that removes genes with zero expression.'''\n        pass\n\n    def get_max_expression(self, gene):\n        '''Placeholder for a function that tells you gene(s) with highest expression.'''\n        pass\n\n\n# setting up object of my custom class Cell, called mycell\ncell1_exp = {'gene1': 2, 'gene2': 3, 'gene3': 0, 'gene4': 8} # dict of expression to input under expression argument of cell.\nmycell = Cell('cell1', cell1_exp, 'brain')\n\n\nprint(mycell.get_expression('gene2'))  # using a method of my custom class Cell - should return 3\nprint(mycell.location)  # accessing an attribute of my custom class Cell - hould return brain\n\n3\nbrain\n\n\nYou want to use classes when you need reusable, modular code - where you want your data to be in a specific format and you want to apply the same functions (methods) to it. This is what the scanpy authors did when building the AnnData class.\nIn the above block, I demonstrate the use of docstrings - so that others and most importantly you, can remember what the function/class is. I have used the ‘numpy’ version of docstrings, the most popular option is the google format. Docstrings can be accessed by using help(my_function_name)"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-4-you-can-turn-your-scripts-with-classesfunctions-into-packages-easily",
    "href": "PythonIntro_SIB.html#tip-4-you-can-turn-your-scripts-with-classesfunctions-into-packages-easily",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 4: You can turn your scripts with Classes/Functions into packages easily",
    "text": "TIP 4: You can turn your scripts with Classes/Functions into packages easily\nWhat is nice, is when you have a series of functions/classes you find yourself using a lot, you can easily package it and import it into other projects of yours.\nLet’s say I saved the above coding block in a file called “cellutils.py”, I would be able to import it into my other scripts using:\n\n# if python cannot find your package put the following line above:\n#sys.path.insert(0, '/path/to/dir/where/your/script/is/')\nimport cellutils as cu\n\ncu.scale_counts(np.array([[1, 2], [3, 4]]))\n\narray([[0.33333333, 0.66666667],\n       [0.42857143, 0.57142857]])\n\n\nNote: I will not go into it here, but it is best practice to put code that runs your script (i.e. everything that is not defining a class or function) within if __name__ == \"__main__\":. Briefly, when you call the script to run from Shell - it will run your entire script (i.e. __name__ = \"__main__\"), when importing it as a package, it will only run your functions/classes defined outside of this if statement (i.e. __name__ != \"__main__\").\nIf you want to run your script as a command line tool, that is also possible and you can look into the package argparse to clearly input arguments e.g.\nin the command line you could have something like:\n\npython myscript.py -a file.txt -s 0.45 --method x2\n\nThe Python script would look something like:\n\n#!/usr/bin/env python3\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Process a file with a given method and scale.\")\n    parser.add_argument(\"-a\", \"--afile\", type=str, required=True, help=\"Input file (e.g., file.txt)\")\n    parser.add_argument(\"-s\", \"--scale\", type=float, required=True, help=\"Scaling factor (e.g., 0.45)\")\n    parser.add_argument(\"--method\", type=str, choices=[\"x1\", \"x2\", \"log\"], default=\"x1\", help=\"Processing method\")\n    \n    args = parser.parse_args()\n\n    print(f\"File: {args.afile}\")\n    print(f\"Scale: {args.scale}\")\n    print(f\"Method: {args.method}\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-5-use-comprehensions-instead-of-for-loops-if-your-output-is-a-list-or-setdictionary",
    "href": "PythonIntro_SIB.html#tip-5-use-comprehensions-instead-of-for-loops-if-your-output-is-a-list-or-setdictionary",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 5: Use comprehensions instead of for loops if your output is a list (or set/dictionary)",
    "text": "TIP 5: Use comprehensions instead of for loops if your output is a list (or set/dictionary)\nIn Python the classic for and while loops exist to go through collection objects (such as lists).\nLet’s say we have a Python list with genes that are found expressed in a cell:\n\nexpressed_genes = [\n    \"Actb\",\n    \"Gapdh\",\n    \"Cd3e\",\n    \"Pax6\",\n    \"Foxp3\",\n    \"Tubb3\",\n    \"mt-Co1\",   # mitochondrial\n    \"mt-Nd2\",   # mitochondrial\n    \"mt-Cytb\",  # mitochondrial\n    \"Il2ra\"\n]\n\n# if we want to get and print the first letter of each gene\nfor gene in expressed_genes: # with this line we are iterating through each item in the list expressed_genes - in each iteration of the loop the item will be under the variable gene\n  print(gene[0]) # in the for loop we are printing the first letter of the string.\n\nA\nG\nC\nP\nF\nT\nm\nm\nm\nI\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can remove a variable in python using del varname.\n\n\nIn Python, as mentioned before, different object types have their own methods (and attributes) associated with them.\nBelow, we have the string \"Actb\" - if we want to see what attributes and methods are available we can use the dir(var) function.\nIt will print all the attributes and methods available.\n\ndir(\"Actb\")\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nIf we want to see what any of these do specifically, we can use the help function as shown below:\n\nhelp(\"Actb\".startswith)\n\nHelp on built-in function startswith:\n\nstartswith(...) method of builtins.str instance\n    S.startswith(prefix[, start[, end]]) -&gt; bool\n    \n    Return True if S starts with the specified prefix, False otherwise.\n    With optional start, test S beginning at that position.\n    With optional end, stop comparing S at that position.\n    prefix can also be a tuple of strings to try.\n\n\n\nNow we know what the startswith method does - lets use it in a for loop to go through the list of expressed genes and identify mitochondrial genes (and store them in a list).\n\nmtgenes = [] # initialising empty list\nfor gene in expressed_genes:    # looping through expressed_genes list\n  if gene.startswith('mt-'):    # if the gene starts with mt-\n    mtgenes.append(gene)            ## let's append it to the mtgenes list we intiated above.\n\n# how to print to console\nprint(mtgenes)\n\n# to print a bit nicer\nprint(\", \".join(mtgenes))\n\n# to print even more clearly, we can use f strings - allows us to be more verbose.\nprint(f'Mitochondrial genes: {\", \".join(mtgenes)}')\nprint(f'There are {len(mtgenes)} mitochondrial genes in the dataset.')\n\n['mt-Co1', 'mt-Nd2', 'mt-Cytb']\nmt-Co1, mt-Nd2, mt-Cytb\nMitochondrial genes: mt-Co1, mt-Nd2, mt-Cytb\nThere are 3 mitochondrial genes in the dataset.\n\n\n\nlist comprehensions\nIn general if you are iterating through a list, and the result of your loop is a list - you should use a list comprehension! The reason to do this is because they are more efficient and quicker to do the same analysis than a loop.\n\n\n\n\n\n\nNote\n\n\n\nThere are also generators, dictionary and set comprehensions - read more about it here.\n\n\nIn this block below, I demonstrate how we can convert the loops shown above into list comprehensions.\n\nfirstletter_genes = [gene[0] for gene in expressed_genes] # what you want in final list, your iterator id, what you are iterating through\n\nmtgenes_lc = [gene for gene in expressed_genes if gene.startswith('mt')] # you can also add if statements in\n\nprint(\", \".join(firstletter_genes))\nprint(\", \".join(mtgenes_lc))\n\nA, G, C, P, F, T, m, m, m, I\nmt-Co1, mt-Nd2, mt-Cytb\n\n\nis it faster??\n\n\nMeasuring the speed of list comprehensions with timeit:\nsetup_code = \"expressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']\"\n\nloop_code = \"\"\"\nmtgenes = [] # initialising empty list\nfor gene in expressed_genes:\n  if gene.startswith('mt-'):\n    mtgenes.append(gene)\n\"\"\"\n\nlist_comp_code = \"[gene for gene in expressed_genes if gene.startswith('mt')]\"\n\n\n# Timing both\nloop_time = timeit.timeit(loop_code, setup=setup_code, number=10000)\nlist_comp_time = timeit.timeit(list_comp_code, setup=setup_code, number=10000)\n\nprint(f\"For loop time: {loop_time:.4f} secs\")\nprint(f\"List comprehension time: {list_comp_time:.4f} secs\")\n\n\nFor loop time: 0.0143 secs\nList comprehension time: 0.0138 secs\n\n\nThe reason is list comprehensions essentially run the iterations at a faster and more efficient level using the underlying C implementation, whereas the loop is having to enter the list each time and append to an unknown sized list.\n\n\nA simple way to measure the speed of your code:\nimport time\n\nt0 = time.time()\n# place code you want to run here!\nt1 = time.time()\n\ntotal = t1-t0 # this will store the time it takes for you to run that code"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-6-sometimes-it-will-be-better-to-use-collection-objects-other-than-lists",
    "href": "PythonIntro_SIB.html#tip-6-sometimes-it-will-be-better-to-use-collection-objects-other-than-lists",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 6: Sometimes it will be better to use collection objects other than lists!",
    "text": "TIP 6: Sometimes it will be better to use collection objects other than lists!\nBeyond lists, other collection objects exist. I will briefly show you sets, tuples and dictionaries.\n\n# Lists are denoted by []   -&gt; ordered, changeable, can hold duplicates\ngenes = [\"Actb\", \"Gapdh\", \"Cd3e\"]\n# Tuples are denoted by ()  -&gt; ordered, unchangeable, can hold duplicates - makes code faster\ngenes = (\"Actb\", \"Gapdh\", \"Cd3e\")\n# Sets are denoted by {}    -&gt; unordered, immutable (can add/remove items though), duplicates not allowed   \ngenes = {\"Actb\", \"Gapdh\", \"Cd3e\"}\n\n# to look up the methods available to your data type you can use the dir(genes) or help(genes) function.\n\n# Dictionaries are mappings of keys to values, allowing you to associate names with data. They are also denoted by {}, the items within are not just a list of items, but key: item.\ngene_expression = {\n    \"Actb\": 7.2,\n    \"Gapdh\": 8.1,\n    \"Cd3e\": 3.4\n}\n\n# you will be able to access any of those variables using your key e.g. gene_expression['Actb'] will return 7.2.\n\n# dictionaries are good to store most object types e.g. you could have a list of genes expressed in key cells\ngenes_expressed = {\n    \"Cell1\": ['gene1', 'gene2'],\n    \"Cell2\": ['gene1', 'gene3', 'gene4'],\n    \"Cell3\": ['gene2', 'gene3']\n}\n\n# you can even have dictionaries of dictionaries!\n# dictionaries are really useful for storing data - you could have multiple dicts e.g. genes_mutated, gene_lengths and you can access the data for a particular sample with the same key e.g. genes_mutated['sample1'] or gene_lengths['sample1'].\n\n# Side note: {} will intialise an enpty dictionary ([] for list and () for tuples). To intialise an empty set, you must use set().\n\nIn general, I use lists. I use sets occasionally, for example, to hold a list of genes that I need to filter out of a dataset (I don’t care about order here, don’t want duplicates, and won’t really be changing it).\nDictionaries I use relatively frequently, especially for storing data/metadata - and if you have a dictionary of (equally sized) lists, it is easy to convert it to a pandas dataframe (which I will go into data layer).\nAnother usecase of a dictionary would be if you have a list of mouse genes and their associated human genes - you will be able to easily convert a list of mouse genes to human genes using the dictionary.\n\nIndexing and Slicing\nBelow, I show some ways to index and slice a list, for your reference.\n\nexpressed_genes = ['Actb','Gapdh', 'Cd3e', 'Pax6', 'Foxp3', 'Tubb3', 'mt-Co1', 'mt-Nd2', 'mt-Cytb', 'Il2ra']\n\nprint(\"first item of genes list:\") # Python is zero-indexed (i.e., indexing starts at 0)!\nexpressed_genes[0]\n\nprint(\"items 1 to 3 of genes list:\")\nexpressed_genes[1:3]\n\nprint(\"last item of genes list:\")\nexpressed_genes[-1]\n\nprint(\"every second item in the genes list:\")\nexpressed_genes[::2]\n\n\nif 'Gapdh' in expressed_genes: \n    print('Gapdh is expressed')\nelse:\n    print('Gapdh is not expressed')\n\nif 'Lyve1' not in expressed_genes: \n    print('Lyve1 is not expressed')\nelif 'Lyve1' in expressed_genes:\n    print('Lyve1 is expressed')\nelse:\n    print('Lyve1 is neither expressed nor not expressed - impossible!')\n\n\nprint('Get length of a variable:')\nlen(expressed_genes)\n\n# methods available for lists:\nexpressed_genes.sort() # it sorts in place, so no need to put it inot a new variable.\n\nexpressed_genes.append('Lyve1') # genes.insert(0, 'Lyve1') if you want to insert it at the beginning\n\nexpressed_genes.remove('Lyve1')\n\nfirst item of genes list:\nitems 1 to 3 of genes list:\nlast item of genes list:\nevery second item in the genes list:\nGapdh is expressed\nLyve1 is not expressed\nGet length of a variable:"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-7-catch-your-errors-using-try-except",
    "href": "PythonIntro_SIB.html#tip-7-catch-your-errors-using-try-except",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 7: Catch your errors using try except",
    "text": "TIP 7: Catch your errors using try except\nSometimes you want to do an operation, but there is an expected error that might pop up - normally an error will cause your programme to exit - you might not want this behaviour. With errors you expect, you can catch the error and do something other than exiting - this is achieved by using try except./ For example, if you have genes you always want to remove from a dataset of expressed genes - you might have a fixed list of genes to remove. However, in a future project, those genes may not be expressed and therefore not in the list of expressed genes -&gt; trying to remove it will raise an error -&gt; you can catch the error and do something else like printing a helpful warning message (or run a different function).\n\ngenes2remove = ['Pins', 'Pard3']\n\n\nfor gene in genes2remove:\n    try:\n        expressed_genes.remove(gene)\n    except ValueError:\n        print(f'{gene} not an expressed_gene, so cannot be removed')\n\nPins not an expressed_gene, so cannot be removed\nPard3 not an expressed_gene, so cannot be removed"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-8-use-pandas-to-read-in-and-handle-dataframes",
    "href": "PythonIntro_SIB.html#tip-8-use-pandas-to-read-in-and-handle-dataframes",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 8: Use pandas to read in and handle DataFrames",
    "text": "TIP 8: Use pandas to read in and handle DataFrames\n\n\nGenerating the tables:\nimport os\n\nnp.random.seed(42)\n\n# generating a 10x10 table with 10 genes and 10 cells\ngenes = [f\"Gene{i}\" for i in range(10)]\ncells = [f\"Cell{i}\" for i in range(10)]\n\n# random count matrix (integers between 50 and 500)\ncount_matrix = np.random.randint(50, 500, size=(10, 10))\ndf = pd.DataFrame(count_matrix, index=genes, columns=cells)\n\n\ndf_KO = df.copy() # creating KO version: copying to modify\n# reducing expression for Gene3 and Gene7 in Cell0, Cell1, Cell2\nfor gene in [\"Gene3\", \"Gene7\"]:\n    for cell in [\"Cell0\", \"Cell1\", \"Cell2\"]:\n        df_KO.loc[gene, cell] = df_KO.loc[gene, cell] // 10  # Strong reduction\n\n\noutput_dir = '/Users/tkafle/Documents/PyBestPractices_SIB/data/' # ensuring output dir exists\nos.makedirs(output_dir, exist_ok=True)\n# saving the files\ndf.to_csv(f\"{output_dir}/WT_1.csv\")\nrandom_changes = np.random.randint(-50, 51, size=df.shape)\nmodified_df = df + random_changes\nmodified_df = modified_df.where(df &gt;= 0, 0) # could alternatively use modified_df.clip(lower=0)\nmodified_df.to_csv(f\"{output_dir}/WT_2.csv\")\ndf_KO.to_csv(f\"{output_dir}/KO_1.csv\")\nrandom_changes = np.random.randint(-50, 51, size=df_KO.shape)\nmodified_df = df_KO + random_changes\nmodified_df = modified_df.where(modified_df &gt;= 0, 0) # could alternatively use modified_df.clip(lower=0)\nmodified_df.to_csv(f\"{output_dir}/KO_2.csv\")\n\n\nThe most commonly used package to handle tables in Python is pandas oftened imported as pd.\nIt is easy to read in a csv into pandas using it’s built in read_csv function:\n\nimport os\noutput_dir = os.path.join(os.getcwd(), 'data')\nwt1_df = pd.read_csv(f\"{output_dir}/WT_1.csv\", index_col=0) # the table is of type: pd.DataFrame\n\nNow you have your data stored in an object of type pd.DataFrame - and of course there are a bunch of methods and attributes you can now access to inspect and process the dataframe. I highlight some examples below.\n\n# Attributes\nprint(\"Shape of dataframe:\")\nprint(wt1_df.shape)    # (10, 10) — 10 genes x 10 cells\n\nprint(\"Column names of dataframe:\")\nprint(wt1_df.columns)  # list of cell names (columns)\n\n# Methods\nprint(\"\\nTransposing dataframe:\")\nprint(wt1_df.transpose().head()) # flip genes and cells\nprint(\"\\nAdding a new column:\")\nwt1_df['GeneSum'] = wt1_df.sum(axis=1) # add a new column: sum of counts per gene\nprint(wt1_df.head())\nwt1_df.drop(['GeneSum'], axis=1, inplace=True) # removing the new column, #inplace lets us do it directly on the df and not create a new variable.\n\n# Change data type (example: ensure all counts are integers)\nwt1_df = wt1_df.astype(int)\n\n# remember you can use dir(pd.DataFrame) and the help() function to better understand the attributes/methods of this object type.\n\nShape of dataframe:\n(10, 10)\nColumn names of dataframe:\nIndex(['Cell0', 'Cell1', 'Cell2', 'Cell3', 'Cell4', 'Cell5', 'Cell6', 'Cell7',\n       'Cell8', 'Cell9'],\n      dtype='object')\n\nTransposing dataframe:\n       Gene0  Gene1  Gene2  Gene3  Gene4  Gene5  Gene6  Gene7  Gene8  Gene9\nCell0    152    264    307     71    495    184    314    480    103    495\nCell1    485    380    393    302    224     70    395     84    155    319\nCell2    398    137    463    285    495    378    102    255    309    400\nCell3    320    422    343    394    100    216    435    130    359    353\nCell4    156    149    435     98    413    323    389    469    240    320\n\nAdding a new column:\n       Cell0  Cell1  Cell2  Cell3  Cell4  Cell5  Cell6  Cell7  Cell8  Cell9  \\\nGene0    152    485    398    320    156    121    238     70    152    171   \nGene1    264    380    137    422    149    409    201    180    199    358   \nGene2    307    393    463    343    435    241    493    326    210    363   \nGene3     71    302    285    394     98    108    219    237    320    239   \nGene4    495    224    495    100    413    104    293    369    180    356   \n\n       GeneSum  \nGene0     2263  \nGene1     2699  \nGene2     3574  \nGene3     2273  \nGene4     3029  \n\n\nBelow, I briefly show ways you can access data in your pd.DataFrame.\n\n# USING NAMES OF COLUMNS AND ROWS\n# accessing the expression of a speficific gene in a specific cell\nwt1_df.loc['Gene0', 'Cell1'] # row name, column name\n\n# accessing the expression values of a specific cell\nwt1_df['Cell1'] # column name\n\n# accessing the expression of a specific row\nwt1_df.loc['Gene0'] # row name\n\n# accessing multiple columns of a specific row\nwt1_df.loc['Gene0', wt1_df.columns.str.startswith('Cell')]\n\n\n# USING INDEX VALUES OF COLUMNS AND ROWS\n# accessing a specific cell \nwt1_df.iloc[0, 1] # row index, column index\n\n# accessing a row\nwt1_df.iloc[4, :] # row index (usually you would simply do: wt1_df.iloc[4])\n\n# accessing a column\nwt1_df.iloc[:, 0] # : means whole row, column index\n\nIn an AnnData object, a lot of the data is stored in pandas dataframes (e.g. var and obs)."
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-9-use-numpy-for-advanced-mathematical-functions",
    "href": "PythonIntro_SIB.html#tip-9-use-numpy-for-advanced-mathematical-functions",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 9: Use NumPy for advanced mathematical functions",
    "text": "TIP 9: Use NumPy for advanced mathematical functions\nYou can perform basic mathematical functions in Python, not dissimilarly to R. Some advanced functions are in numpy (usually imported as np). Basic mathematical functions:\n\n# Mathematical functions available directly in Python\n# addition\n_ = 5 + 4\n\n# subtraction\n_ = 5 - 4\n\n# multiplication\n_ = 5 * 4\n\n# division\n_ =  5 / 2\n\n# squaring\n_ =  5 ** 2\n\n# getting remainder (modulo)\n_ =  5 % 2\n\n# Advanced functions in Python are largely found in packages such as numpy [np]\n\n# square root\n_ = np.sqrt(16)  # or math.sqrt(16)\n\n# natural logarithm (log base e)\n_ = np.log(10)  # or math.log(10)\n\n# logarithm base 10\n_ = np.log10(100)\n\n# exponentiation (e^x)\n_ = np.exp(4)\n\n# absolute value\n_ = np.abs(-5)\n\n# rounding numbers\n_ = np.round(3.567, 2)  # rounds to 2 decimal places\n\n\n\n# I will note here, when printing long floats, f strings are really useful for formatting them.\n\nnum = 3.1415926535\n# round to 3 decimal places\nprint(f\"Pi rounded to 3 decimals: {num:.3f}\") # f means \n# round to 1 decimal place\nprint(f\"Pi rounded to 1 decimal: {num:.1f}\")\n\nsml_num = 0.00123456\n# 3 significant figures in decimals\nprint(f\"{sml_num:.3g}\") #g means general format\nbig_num = 123456\n# 3 significant figures (scientific notication)\nprint(f\"{big_num:.3g}\")\n\nPi rounded to 3 decimals: 3.142\nPi rounded to 1 decimal: 3.1\n0.00123\n1.23e+05\n\n\nRemember when printing floats:\n- .3f = 3 decimal places. - .3g = 3 significant figures.\nnp is also better for doing mathematical functions across vectors and things like matrix maths."
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-10-several-packages-exist-for-plotting-seaborn-and-matplotlib-can-probably-do-most-of-what-you-want.",
    "href": "PythonIntro_SIB.html#tip-10-several-packages-exist-for-plotting-seaborn-and-matplotlib-can-probably-do-most-of-what-you-want.",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 10: Several packages exist for plotting: seaborn and matplotlib can probably do most of what you want.",
    "text": "TIP 10: Several packages exist for plotting: seaborn and matplotlib can probably do most of what you want.\nIn general there are three popular libraries:\n- Matplotlib (basic, customisable, but more work needed to build plots) - most similar to base R plotting - Seaborn (easy, beautiful defaults) - most similar to ggplot2 - Plotly (interactive) - also exists in R!\nThe dataset I will use is the data I generated earlier in this document - it contains 2 samples of KO and 2 samples of WT with count data from 10 genes and 10 cells.\n\n\nReading in the dataset:\n# load the datasets\nwt1 = pd.read_csv(f\"{output_dir}/WT_1.csv\", index_col=0)\nwt2 = pd.read_csv(f\"{output_dir}/WT_2.csv\", index_col=0)\nko1 = pd.read_csv(f\"{output_dir}/KO_1.csv\", index_col=0)\nko2 = pd.read_csv(f\"{output_dir}/KO_2.csv\", index_col=0)\n\n# stack into one DataFrame for easier plotting\n# adding group labels\nwt1['group'] = 'WT'\nwt2['group'] = 'WT'\nko1['group'] = 'KO'\nko2['group'] = 'KO'\nwt1['id'] = 'WT1'\nwt2['id'] = 'WT2'\nko1['id'] = 'KO1'\nko2['id'] = 'KO2'\n\n# Combine into one DataFrame\ndf = pd.concat([wt1, wt2, ko1, ko2])\n\nprint(df.head())\n\n\n       Cell0  Cell1  Cell2  Cell3  Cell4  Cell5  Cell6  Cell7  Cell8  Cell9  \\\nGene0    152    485    398    320    156    121    238     70    152    171   \nGene1    264    380    137    422    149    409    201    180    199    358   \nGene2    307    393    463    343    435    241    493    326    210    363   \nGene3     71    302    285    394     98    108    219    237    320    239   \nGene4    495    224    495    100    413    104    293    369    180    356   \n\n      group   id  \nGene0    WT  WT1  \nGene1    WT  WT1  \nGene2    WT  WT1  \nGene3    WT  WT1  \nGene4    WT  WT1  \n\n\n\nmatplotlib.pyplot (plt)\nHere, I will show how to make a plot and layer using matplotlib.pyplot (plt). The plot will be a histogram showing the expression of genes in Cell2 for sample 1.\nmatplotlib.pyplot is the underlying library that seaborn builds on top of. You can keep layering plots onto the same figure until you call plt.show(), which renders the final output. If you’d like to create multiple plots arranged in a grid, you can follow this helpful guide.\n\nimport matplotlib.pyplot as plt\n\ncell2_wt1_expression =  df[df.id == 'WT1']['Cell2']\nplt.figure(figsize=(6,4)) # initating figure size\nplt.hist(cell2_wt1_expression, bins=5, color='skyblue', edgecolor='black') # plotting the histogram\nplt.title('Distribution of Gene_1 Expression') # title of plot\nplt.xlabel('Expression') # x axis label\nplt.ylabel('Count') # y axis label\nplt.grid(True)  # layering on grid\nplt.axvline(cell2_wt1_expression.mean(), color='red', linestyle='dashed', linewidth=2, label='Mean')  # placing vertical line to show where the mean is\nplt.legend() # plot the legend\nplt.show() # show the plot\n\n\n\n\n\n\n\n\nplt.show() is what you need to do to show the plot in a popup window. If you want to save the figure there is plt.savefig().\nOther good practices are to close figures once they have been initiated using plt.close() or plt.clf().\n\n\nseaborn (sns)\nseaborn usually runs on long data, so we will need to melt the pandas dataframe from before.\n\n\nMelting the dataset:\n# melting each DF\nif 'gene' not in wt1.columns:\n    wt1.reset_index(inplace=True)\n    wt1.rename(columns={'index': 'gene'}, inplace=True)\nwt1_melted = wt1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in wt2.columns:\n    wt2.reset_index(inplace=True)\n    wt2.rename(columns={'index': 'gene'}, inplace=True)\nwt2_melted = wt2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in ko1.columns:\n    ko1.reset_index(inplace=True)\n    ko1.rename(columns={'index': 'gene'}, inplace=True)\nko1_melted = ko1.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\nif 'gene' not in ko2.columns:\n    ko2.reset_index(inplace=True)\n    ko2.rename(columns={'index': 'gene'}, inplace=True)\nko2_melted = ko2.melt(id_vars=['group', 'id', 'gene'], var_name='cell', value_name='expression')\n\n\n# Merge the DataFrames\nmerged_df = pd.concat([wt1_melted, wt2_melted, ko1_melted, ko2_melted], ignore_index=True)\n#merged_df['gene'] = merged_df['gene'].apply(lambda x: 'Gene' + str(int(x)))\n\nprint(merged_df.head())\n\n\n  group   id   gene   cell  expression\n0    WT  WT1  Gene0  Cell0         152\n1    WT  WT1  Gene1  Cell0         264\n2    WT  WT1  Gene2  Cell0         307\n3    WT  WT1  Gene3  Cell0          71\n4    WT  WT1  Gene4  Cell0         495\n\n\nSeaborn is typically imported as sns.\nHere, I show an example using seaborn - just requires the long form pd.DataFrame, and you can easily plot the x and y axis based on specific columns. Here, I am plotting a violin plot using sns.violinplot, other plot types can be found here.\nimport seaborn as sns\n\n# Gene1\nplt.figure(figsize=(6,4))\nsns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene1'], palette='muted')\nplt.title('Gene1 Expression by Group')\nplt.show()\n\n#Gene3\nplt.figure(figsize=(6,4))\nsns.violinplot(x='group', y='expression', data=merged_df[merged_df.gene == 'Gene3'], palette='muted')\nplt.title('Gene3 Expression by Group')\nplt.show()\n\n\n\n\n\n\nViolin plot: gene 1\n\n\n\n\n\n\n\nViolin plot gene 3\n\n\n\n\n\nIn this example, I am layering using seaborn - plotting a swarmplot above the catplot. I store the catplot in the variable g, I then plot the swarmplot on the same axes by using ax=g.ax.\nimport seaborn as sns\n\n# Gene2\ng = sns.catplot(data=merged_df[merged_df.gene == 'Gene2'], x=\"group\", y=\"expression\",\n                kind=\"violin\", color=\".9\", inner=None, height=4, aspect=1.5)\nsns.swarmplot(data=merged_df[merged_df.gene == 'Gene2'], x=\"group\", y=\"expression\", size=3, ax=g.ax)\ng.ax.set_title('Gene2 Expression by Group')\nplt.show()\n\n# gene7\ng = sns.catplot(data=merged_df[merged_df.gene == 'Gene7'], x=\"group\", y=\"expression\",\n                kind=\"violin\", color=\".9\", inner=None, height=4, aspect=1.5)\nsns.swarmplot(data=merged_df[merged_df.gene == 'Gene7'], x=\"group\", y=\"expression\", size=3, ax=g.ax)\ng.ax.set_title('Gene7 Expression by Group')\nplt.show()\n\n\n# you can access the data in the sns.FacetGrid object (variable g in my script) by calling g.data\n\n\n\n\n\n\nViolin plot: gene 2\n\n\n\n\n\n\n\nViolin plot gene 7\n\n\n\n\n\nIn the example below I use sns.scatterplot to plot a volcano plot. I generate some fake data.\nI use seaborn for the scatter plot, and then layer on top of it using matplotlib.pyplot (plt).\n\n# creating fake log2 fold change and p-value data\nvolcano_data = pd.DataFrame({\n    #'gene': ['Gene0', 'Gene1', 'Gene2', 'Gene3', 'Gene4', 'Gene5', 'Gene6', 'Gene7', 'Gene8', 'Gene9'],\n    'log2FC': np.random.randn(1000),  # Random fold changes\n    'pval': np.random.rand(1000)  # Random p-values\n})\n\n# calculating -log10(p-value)\nvolcano_data['-log10(pval)'] = -np.log10(volcano_data['pval'])\n\nplt.figure(figsize=(6,5))\nsns.scatterplot(data=volcano_data, x='log2FC', y='-log10(pval)', color='purple')\nplt.axhline(y=1.3, color='red', linestyle='dashed')  # threshold line for p-value = 0.05\nplt.axvline(x=0, color='black', linestyle='dotted')\nplt.title('Volcano Plot (Mock Example)')\nplt.xlabel('Log2 Fold Change')\nplt.ylabel('-Log10(p-value)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplotly\nPlotly exists in R as well, and is good for interactivity - widget-like plots. You can hover over points and filter points by clicking on the legend.\nplotly express functions are not too dissimilar to using seabron - in the example below I use px.strip to generate a strip plot. It requires long data format pd.DataFrame, and you plot simply by putting the columns you want to plot under x and y (columns group and expression, respectively). The colour of each point is based on the column cell.\n\nimport plotly.express as px\n\n# Plotly: Strip Plot (interactive)\nfig = px.strip(merged_df[merged_df.gene == 'Gene3'], x='group', y='expression', color='cell', stripmode='overlay', title='Gene Expression by Cell (WT vs KO)')\nfig.show()\n\n# the variable fig is a plotly.graph_objects.Figure"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-11-you-can-do-your-statistical-tests-in-python-using-scipy.stats-or-statsmodels",
    "href": "PythonIntro_SIB.html#tip-11-you-can-do-your-statistical-tests-in-python-using-scipy.stats-or-statsmodels",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 11: You can do your statistical tests in python using scipy.stats or statsmodels",
    "text": "TIP 11: You can do your statistical tests in python using scipy.stats or statsmodels\nR is known for being able to carry out stats tests, but through packages such as statsmodels and scipy.stats, we have access to many statistical tests that you need for scientific studies.\nI will show you some examples, these tests are shown only for demonstration; there’s no real hypothesis being tested.\n\nGLM\n\nimport statsmodels.api as sm\n\n\n#merged_df['group_binary'] = merged_df['group'].apply(lambda x: 1 if x == 'KO' else 0) # converting KO/WT to binary 0/1.\n\n# for this example, we will just put gene expression as a continuous response\nX = pd.get_dummies(merged_df['group'], drop_first=True)  # one-hot encoding for group variable\nX = sm.add_constant(X)  # add intercept\ny = merged_df['expression']\n\n# Fit GLM\nmodel = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n\n# Show the results\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             expression   No. Observations:                  400\nModel:                            GLM   Df Residuals:                      398\nModel Family:                Gaussian   Df Model:                            1\nLink Function:               identity   Scale:                          17963.\nMethod:                          IRLS   Log-Likelihood:                -2525.8\nDate:                Wed, 30 Apr 2025   Deviance:                   7.1492e+06\nTime:                        15:39:58   Pearson chi2:                 7.15e+06\nNo. Iterations:                     3   Pseudo R-squ. (CS):           0.002870\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        269.2350      9.477     28.409      0.000     250.660     287.810\nWT            14.3400     13.403      1.070      0.285     -11.928      40.608\n==============================================================================\n\n\n\n\nT-test\n\nfrom scipy import stats\n\n# two conditions WT vs KO, different in expression\nko_expr = merged_df[merged_df['group'] == 'KO']['expression']\nwt_expr = merged_df[merged_df['group'] == 'WT']['expression']\n\n# performing two-sample t-test\nt_stat, p_value = stats.ttest_ind(ko_expr, wt_expr)\n\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nT-statistic: -1.069947991397797, P-value: 0.2852911632028579\n\n\n\n\nChi2 test\n\n# categorising gene expression into 'high' or 'low' based on median expression\nmedian_expr = merged_df['expression'].median()\nmerged_df['expr_category'] = merged_df['expression'].apply(lambda x: 'high' if x &gt; median_expr else 'low')\n\n# creating a contingency table for chisq test\ncontingency = pd.crosstab(merged_df['expr_category'], merged_df['group'])\n\n# performing the Chi-Squared test\nchi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n\nprint(f\"Chi-Squared: {chi2}, P-value: {p_val}\")\n\nChi-Squared: 0.4900490049004901, P-value: 0.4839054452991407\n\n\n\n\nANOVA\n\n# performing ANOVA on expression values grouped by the 'group' column\ngroup1 = merged_df[merged_df['group'] == 'WT']['expression']\ngroup2 = merged_df[merged_df['group'] == 'KO']['expression']\n\nf_stat, p_val = stats.f_oneway(group1, group2)\nprint(f\"ANOVA F-statistic: {f_stat}, P-value: {p_val}\")\n\nANOVA F-statistic: 1.144788704296184, P-value: 0.28529116320296\n\n\n\n\nRunning correction on multiple testing\n\nimport pandas as pd\nimport scipy.stats as stats\nfrom statsmodels.stats.multitest import multipletests\n\n\n# creating a list to store p-values - we will run multiple test correction\np_values = []\n\n# iterating over each gene (rows of the dataframe) to get expression for KO and WT\nfor g in merged_df.gene.unique():\n    # getting expression data for the gene in both KO and WT groups\n    ko_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'KO')]['expression'].to_numpy()\n    wt_expr = merged_df[(merged_df.gene == g) & (merged_df.group == 'WT')]['expression'].to_numpy()\n\n    # performing two-sample t-test on each gene expression data\n    t_stat, p_val = stats.ttest_ind(ko_expr, wt_expr, equal_var=False)\n    p_values.append(p_val)\n\n# Now we will apply multiple testing correction to the p-values using Benjamini-Hochberg (FDR) method, other methods such as Bonferroni are available\nreject, corrected_pvals, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n\n# Create a dataframe to view the results\nresults = pd.DataFrame({\n    'gene': merged_df.gene.unique(),\n    'p-value': p_values,\n    'corrected p-value': corrected_pvals,\n    'reject null hypothesis': reject\n})\n\nprint(results)\n\n    gene   p-value  corrected p-value  reject null hypothesis\n0  Gene0  0.935963           0.995188                   False\n1  Gene1  0.951398           0.995188                   False\n2  Gene2  0.673559           0.995188                   False\n3  Gene3  0.106674           0.995188                   False\n4  Gene4  0.816119           0.995188                   False\n5  Gene5  0.864392           0.995188                   False\n6  Gene6  0.995188           0.995188                   False\n7  Gene7  0.224214           0.995188                   False\n8  Gene8  0.886540           0.995188                   False\n9  Gene9  0.667638           0.995188                   False"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-12-you-can-parallelise-your-code-using-multiprocessing",
    "href": "PythonIntro_SIB.html#tip-12-you-can-parallelise-your-code-using-multiprocessing",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 12: You can parallelise your code using multiprocessing",
    "text": "TIP 12: You can parallelise your code using multiprocessing\n\nimport time\nimport multiprocessing\n\n\ndef calculate_square(number):\n    '''\n    function to calculate square of a number (CPU-bound task)\n    '''\n    return number * number\n\n\ndef parallel_square(numbers):\n    '''\n    function to run the calculation in parallel using multiple processes\n    '''\n    # you need to create a pool of worker processes\n    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool: # line that lets you access your cpus to run script\n        result = pool.map(calculate_square, numbers)\n    return result\n\ndef single_threaded_square(numbers):\n    '''\n    single-threaded version: Calculating squares sequentially\n    '''\n    result = []\n    for number in numbers:\n        result.append(calculate_square(number))\n    return result\n\nif __name__ == \"__main__\":\n    numbers = [x for x in range(1, 10000001)]  # 10 million numbers\n    \n    # measuring the time for the single-threaded execution\n    start_time = time.time()\n    single_result = single_threaded_square(numbers)\n    single_thread_time = time.time() - start_time\n    print(f\"Single-threaded time: {single_thread_time:.2f} seconds\")\n    \n    # measuring the time for the parallel execution\n    start_time = time.time()\n    parallel_result = parallel_square(numbers)\n    parallel_time = time.time() - start_time\n    print(f\"Parallel execution time: {parallel_time:.2f} seconds\")\n    \n    # comparing results to make sure they are the same\n    print(f\"Do the results match? {'Yes' if single_result == parallel_result else 'No'}\")\n\nSingle-threaded time: 1.24 seconds\nParallel execution time: 1.88 seconds\nDo the results match? Yes"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-13-you-can-generate-pipelines-using-bashpythonr-with-snakemake",
    "href": "PythonIntro_SIB.html#tip-13-you-can-generate-pipelines-using-bashpythonr-with-snakemake",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 13: You can generate pipelines using bash/python/r with Snakemake",
    "text": "TIP 13: You can generate pipelines using bash/python/r with Snakemake\nshow basic snakemake document with three rules: cellranger [bash], preprocessing [python], plotting [r]\n\n# Snakefile\n\n# Define the rule for Cell Ranger processing\nrule cellranger:\n    input:\n        r1=\"data/{sample}_R1.fastq.gz\",\n        r2=\"data/{sample}_R2.fastq.gz\"\n    output:\n        \"output/cellranger_output/{sample}/filtered_feature_bc_matrix\"\n    shell:\n        \"cellranger count --id={wildcards.sample} \"\n        \"--fastqs={input.r1},{input.r2} \"\n        \"--transcriptome=/path/to/refdata-cellranger-mm10-3.0.0 \"\n        \"--sample={wildcards.sample}\"\n\n# Define the rule for Python data processing\nrule process_data:\n    input:\n        \"output/cellranger_output/{sample}/filtered_feature_bc_matrix/matrix.mtx\"\n    output:\n        \"output/analysis_results/{sample}_processed.csv\"\n    script:\n        \"scripts/process_data.py\"\n\n# Define the rule for generating plots using R\nrule plot_data:\n    input:\n        \"output/analysis_results/{sample}_processed.csv\"\n    output:\n        \"output/analysis_results/{sample}_plot.png\"\n    script:\n        \"scripts/plot_data.R\"\n\nrun workflow with snakemake --cores 4 -s Snakefile"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-14-use-type-hinting-to-write-clearly-usable-functions-and-unit-testing-to-ensure-your-programmes-work-as-expected.",
    "href": "PythonIntro_SIB.html#tip-14-use-type-hinting-to-write-clearly-usable-functions-and-unit-testing-to-ensure-your-programmes-work-as-expected.",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 14: Use type hinting to write clearly usable functions and unit testing to ensure your programmes work as expected.",
    "text": "TIP 14: Use type hinting to write clearly usable functions and unit testing to ensure your programmes work as expected.\nIn the example below, it shows the function normalise requires the argument x and this should be of type np.ndarray, and the -&gt; np.ndarray, shows the expected output should be of type np.ndarray - helping you or someone else know what to expect from this function.\n\ndef normalise(x: np.ndarray) -&gt; np.ndarray:\n    return x / x.sum()\n\nOther common types: int, float, list, tuple, range, str, set, dict, bool, NoneType, pd.Series, pd.DataFrame, np.ndarray, np.int64, np.float64, sc.AnnData.\nUnit testing is a crucial practice, especially when your code is intended for use by others or will be reused in the future. It involves writing small tests that check whether individual functions behave as expected given specific inputs. These tests help catch bugs early, document expected behavior, and ensure that future changes don’t break existing functionality. Implementing unit tests improves code reliability and maintainability. You can use unittest or pytest for this.\n\nimport unittest\nimport numpy as np\n\nclass TestNormaliseFunction(unittest.TestCase):\n\n    def test_normalise_single_dimension(self):\n        \"\"\"Test normalization of a 1D array.\"\"\"\n        x = np.array([1, 2, 3])\n        expected = np.array([0.16666667, 0.33333333, 0.5])  # sum = 6, so divide each element by 6\n        result = normalise(x)\n        np.testing.assert_array_almost_equal(result, expected, decimal=8)\n    \n    def test_normalise_multiple_dimension(self):\n        \"\"\"Test normalization of a 2D array.\"\"\"\n        x = np.array([[1, 2], [3, 4]])\n        expected = np.array([[0.1, 0.2], [0.3, 0.4]])  # sum = 10, divide each element by 10\n        result = normalise(x)\n        np.testing.assert_array_almost_equal(result, expected, decimal=8)\n\n    def test_normalise_edge_case(self):\n        \"\"\"Test normalization of an edge case (empty array).\"\"\"\n        x = np.array([])\n        with self.assertRaises(ValueError):  # expecting a ValueError if sum is 0\n            normalise(x)\n\nif __name__ == \"__main__\":\n    unittest.main()"
  },
  {
    "objectID": "PythonIntro_SIB.html#tip-15-use-ipython-when-writing-your-code-to-more-easily-test-it-and-debug-it.",
    "href": "PythonIntro_SIB.html#tip-15-use-ipython-when-writing-your-code-to-more-easily-test-it-and-debug-it.",
    "title": "A Gentle Introduction to Working in Python",
    "section": "TIP 15: Use IPython when writing your code to more easily test it and debug it.",
    "text": "TIP 15: Use IPython when writing your code to more easily test it and debug it.\nIPython allows you to interactively code, so you can test your code in your terminal."
  },
  {
    "objectID": "PythonIntro_SIB.html#r-python-together",
    "href": "PythonIntro_SIB.html#r-python-together",
    "title": "A Gentle Introduction to Working in Python",
    "section": "R & Python together",
    "text": "R & Python together\n\nIn Python you can do a lot of the vectorisation programming you do in R using numpy and pandas.\nPython will have a bit of a learning curve for plots/stats - some R only packages (especially scientific) may not be available.\nUse both! (Snakemake helps bridge them when building pipelines)"
  },
  {
    "objectID": "PythonIntro_SIB.html#working-with-anndata-objects",
    "href": "PythonIntro_SIB.html#working-with-anndata-objects",
    "title": "A Gentle Introduction to Working in Python",
    "section": "Working with AnnData objects",
    "text": "Working with AnnData objects\nIn a later document, I will demonstrate how to analyse single-cell RNAseq data using Python."
  }
]